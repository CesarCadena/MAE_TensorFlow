{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from process_data import process_data\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "batch_size=128\n",
    "hidden_size=1024\n",
    "num_epoch=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAG='train'\n",
    "\n",
    "if (FLAG=='train'):\n",
    "    data=process_data('training')\n",
    "    Red_data=data['Red']\n",
    "    Green_data=data['Green']\n",
    "    Blue_data=data['Blue']\n",
    "    Depth_data=data['Depth']\n",
    "    Depthmask_data=data['Depthmask']\n",
    "    Ground_data=data['Ground']\n",
    "    Objects_data=data['Objects']\n",
    "    Building_data=data['Building']\n",
    "    Vegetation_data=data['Vegetation']\n",
    "    Sky_data=data['Sky']\n",
    "    \n",
    "elif(FLAG=='validation'):\n",
    "    data_val=process_data('validation')\n",
    "    Red_data_val=data_val['Red']\n",
    "    Green_data_val=data_val['Green']\n",
    "    Blue_data_val=data_val['Blue']\n",
    "    Depth_data_val=data_val['Depth']\n",
    "    Depthmask_data_val=data_val['Depthmask']\n",
    "    Ground_data_val=data_val['Ground']\n",
    "    Objects_data_val=data_val['Objects']\n",
    "    Building_data_val=data_val['Building']\n",
    "    Vegetation_data_val=data_val['Vegetation']\n",
    "    Sky_data_val=data_val['Sky']\n",
    "    \n",
    "elif(FLAG=='test'):\n",
    "    data_test=process_data('test')\n",
    "    Red_data_test=data_test['Red']\n",
    "    Green_data_test=data_test['Green']\n",
    "    Blue_data_test=data_test['Blue']\n",
    "    Depth_data_test=data_test['Depth']\n",
    "    Depthmask_data_test=data_test['Depthmask']\n",
    "    Ground_data_test=data_test['Ground']\n",
    "    Objects_data_test=data_test['Objects']\n",
    "    Building_data_test=data_test['Building']\n",
    "    Vegetation_data_test=data_test['Vegetation']\n",
    "    Sky_data_test=data_test['Sky']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Depth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "Red_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Red_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                  stddev=0.01),name=\"Red_weights\")\n",
    "Red_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Red_bias\")\n",
    "\n",
    "Red_hidden=tf.nn.relu(tf.matmul(Red_input,Red_weights)+Red_bias)\n",
    "\n",
    "Red_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01),name=\"Red_outweights\")\n",
    "Red_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Red_outbias\")\n",
    "Red_out=tf.nn.sigmoid(tf.matmul(Red_hidden,Red_outweights)+Red_outbias)\n",
    "\n",
    "\n",
    "loss_red=tf.nn.l2_loss(Red_input-Red_out)\n",
    "\n",
    "regularization_red=(tf.nn.l2_loss(Red_weights)+tf.nn.l2_loss(Red_bias)\n",
    "               +tf.nn.l2_loss(Red_outweights)+tf.nn.l2_loss(Red_outbias))\n",
    "\n",
    "loss_red=loss_red+1e-5*regularization_red\n",
    "\n",
    "optimizer_red=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_red)\n",
    "\n",
    "\n",
    "# ## green channel\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "Green_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Green_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                  stddev=0.01),name=\"Green_weights\")\n",
    "Green_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Green_bias\")\n",
    "\n",
    "Green_hidden=tf.nn.relu(tf.matmul(Green_input,Green_weights)+Green_bias)\n",
    "\n",
    "Green_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01),name=\"Green_outweights\")\n",
    "Green_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Green_outbias\")\n",
    "Green_out=tf.nn.sigmoid(tf.matmul(Green_hidden,Green_outweights)+Green_outbias)\n",
    "\n",
    "\n",
    "loss_green=tf.nn.l2_loss(Green_input-Green_out)\n",
    "\n",
    "regularization_green=(tf.nn.l2_loss(Green_weights)+tf.nn.l2_loss(Green_bias)\n",
    "               +tf.nn.l2_loss(Green_outweights)+tf.nn.l2_loss(Green_outbias))\n",
    "\n",
    "loss_green=loss_green+1e-5*regularization_green\n",
    "\n",
    "optimizer_green=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_green)\n",
    "\n",
    "\n",
    "# ## blue channel\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "Blue_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Blue_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                  stddev=0.01),name=\"Blue_weights\")\n",
    "\n",
    "Blue_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Blue_bias\")\n",
    "\n",
    "Blue_hidden=tf.nn.relu(tf.matmul(Blue_input,Blue_weights)+Blue_bias)\n",
    "\n",
    "Blue_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01),name=\"Blue_outweights\")\n",
    "Blue_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Blue_outbias\")\n",
    "Blue_out=tf.nn.sigmoid(tf.matmul(Blue_hidden,Blue_outweights)+Blue_outbias)\n",
    "\n",
    "\n",
    "loss_blue=tf.nn.l2_loss(Blue_input-Blue_out)\n",
    "\n",
    "regularization_blue=(tf.nn.l2_loss(Blue_weights)+tf.nn.l2_loss(Blue_bias)\n",
    "               +tf.nn.l2_loss(Blue_outweights)+tf.nn.l2_loss(Blue_outbias))\n",
    "\n",
    "loss_blue=loss_blue+1e-5*regularization_blue\n",
    "\n",
    "optimizer_blue=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_blue)\n",
    "\n",
    "\n",
    "# ## depth channel\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "Depth_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Depthmask_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "\n",
    "Depth_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                  stddev=0.01),name=\"Depth_weights\")\n",
    "\n",
    "Depth_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Depth_bias\")\n",
    "\n",
    "Depth_hidden=tf.nn.relu(tf.matmul(Depth_input,Depth_weights)+Depth_bias)\n",
    "\n",
    "\n",
    "Depth_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01),name=\"Depth_outweights\")\n",
    "Depth_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Depth_outbias\")\n",
    "Depth_out=tf.matmul(Depth_hidden,Depth_outweights)+Depth_outbias\n",
    "\n",
    "\n",
    "loss_depth=tf.nn.l2_loss(np.multiply((Depth_input-Depth_out),Depthmask_input) )\n",
    "\n",
    "regularization_depth=(tf.nn.l2_loss(Depth_weights)+tf.nn.l2_loss(Depth_bias)\n",
    "               +tf.nn.l2_loss(Depth_outweights)+tf.nn.l2_loss(Depth_outbias))\n",
    "\n",
    "loss_depth=loss_depth+1e-5*regularization_depth\n",
    "\n",
    "optimizer_depth=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  train  RGBdepth model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretraining finished\n"
     ]
    }
   ],
   "source": [
    "#train RGB and depth weights \n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "train_size=Red_data.shape[0]\n",
    "train_indices=range(train_size)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "\n",
    "par_path=\"../par/\"\n",
    "if not os.path.isdir(par_path):\n",
    "    os.mkdir(par_path)\n",
    "\n",
    "model_path=\"../model\"\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        perm_indices=np.random.permutation(train_indices)\n",
    "        \n",
    "        for step in range(int(train_size/batch_size)):\n",
    "            \n",
    "            \n",
    "            offset=(step*batch_size)%(train_size-batch_size)\n",
    "            batch_indices=perm_indices[offset:(offset+batch_size)]\n",
    "            \n",
    "            \n",
    "            feed_dict={#Ground_input:Ground_data[batch_indices,:],\n",
    "                       #Objects_input:Objects_data[batch_indices,:],\n",
    "                       #Vegetation_input:Vegetation_data[batch_indices,:],\n",
    "                       #Building_input:Building_data[batch_indices,:],\n",
    "                       #Sky_input:Sky_data[batch_indices,:],\n",
    "                       Red_input:Red_data[batch_indices,:],\n",
    "                       Green_input:Green_data[batch_indices,:],\n",
    "                       Blue_input:Blue_data[batch_indices,:],\n",
    "                       Depth_input:Depth_data[batch_indices,:],\n",
    "                       Depthmask_input:Depthmask_data[batch_indices,:]\n",
    "                      }   \n",
    "            \n",
    "            _red,l_red=sess.run([optimizer_red,loss_red],feed_dict=feed_dict)\n",
    "            \n",
    "            _green,l_green=sess.run([optimizer_green,loss_green],feed_dict=feed_dict)\n",
    "            \n",
    "            _blue,l_blue=sess.run([optimizer_blue,loss_blue],feed_dict=feed_dict)\n",
    "            \n",
    "            _depth,l_depth=sess.run([optimizer_depth,loss_depth],feed_dict=feed_dict)\n",
    "     \n",
    "\n",
    "        \n",
    "    saver.save(sess,model_path+\"/pretraining_rgbd.ckpt\")\n",
    "    \n",
    "    \n",
    "    print ('pretraining finished')\n",
    "    np.save(par_path+\"Red_weights\",sess.run(Red_weights))\n",
    "    np.save(par_path+\"Red_bias\",sess.run(Red_bias))\n",
    "    np.save(par_path+\"Red_outweights\",sess.run(Red_outweights))\n",
    "    np.save(par_path+\"Red_outbias\",sess.run(Red_outbias))\n",
    "    \n",
    "    np.save(par_path+\"Green_weights\",sess.run(Green_weights))\n",
    "    np.save(par_path+\"Green_bias\",sess.run(Green_bias))\n",
    "    np.save(par_path+\"Green_outweights\",sess.run(Green_outweights))\n",
    "    np.save(par_path+\"Green_outbias\",sess.run(Green_outbias))\n",
    "    \n",
    "    np.save(par_path+\"Blue_weights\",sess.run(Blue_weights))\n",
    "    np.save(par_path+\"Blue_bias\",sess.run(Blue_bias))\n",
    "    np.save(par_path+\"Blue_outweights\",sess.run(Blue_outweights))\n",
    "    np.save(par_path+\"Blue_outbias\",sess.run(Blue_outbias))\n",
    "    \n",
    "\n",
    "    np.save(par_path+\"Depth_weights\",sess.run(Depth_weights))\n",
    "    np.save(par_path+\"Depth_bias\",sess.run(Depth_bias))\n",
    "    np.save(par_path+\"Depth_outweights\",sess.run(Depth_outweights))\n",
    "    np.save(par_path+\"Depth_outbias\",sess.run(Depth_outbias))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate RGBD channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/pretraining_rgbd.ckpt\n",
      "[[ 0.27215427  0.2357998   0.2074043  ...,  0.72227156  0.77371055\n",
      "   0.76770639]]\n"
     ]
    }
   ],
   "source": [
    "data_val=process_data('validation')\n",
    "Red_data_val=data_val['Red']\n",
    "Green_data_val=data_val['Green']\n",
    "Blue_data_val=data_val['Blue']\n",
    "Depth_data_val=data_val['Depth']\n",
    "Depthmask_data_val=data_val['Depthmask']\n",
    "Ground_data_val=data_val['Ground']\n",
    "Objects_data_val=data_val['Objects']\n",
    "Building_data_val=data_val['Building']\n",
    "Vegetation_data_val=data_val['Vegetation']\n",
    "Sky_data_val=data_val['Sky']\n",
    "    \n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    saver.restore(sess,\"../model/pretraining_rgbd.ckpt\")\n",
    "    \n",
    "    for i in range(0,1):#len(Red_data_val)):\n",
    "        \n",
    "        feed_dict={Red_input:Red_data_val[i:i+1,:],\n",
    "                   Green_input:Green_data_val[i:i+1,:],\n",
    "                   Blue_input:Blue_data_val[i:i+1,:],\n",
    "                   Depth_input:Depth_data_val[i:i+1,:],\n",
    "                   Depthmask_input:Depthmask_data_val[i:i+1,:],\n",
    "                  }\n",
    "        \n",
    "        print(sess.run(Red_out,feed_dict=feed_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  seperate semantic  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ground channel\n",
    "Ground_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "\n",
    "Ground_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                   stddev=0.01),name=\"Ground_weights\")\n",
    "\n",
    "Ground_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Ground_bias\")\n",
    "\n",
    "Ground_hidden=tf.nn.relu(tf.matmul(Ground_input,Ground_weights)+Ground_bias)\n",
    "\n",
    "Ground_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],\n",
    "                                   stddev=0.01),name=\"Ground_outweights\")\n",
    "\n",
    "Ground_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Ground_outbias\")\n",
    "Ground_out=tf.nn.sigmoid(tf.matmul(Ground_hidden,Ground_outweights)+Ground_outbias)\n",
    "\n",
    "\n",
    "loss_Ground=tf.nn.l2_loss(Ground_input-Ground_out)\n",
    "regularization_Ground=(tf.nn.l2_loss(Ground_weights)+tf.nn.l2_loss(Ground_bias)\n",
    "               +tf.nn.l2_loss(Ground_outweights)+tf.nn.l2_loss(Ground_outbias))\n",
    "loss_Ground=loss_Ground+1e-5*regularization_Ground\n",
    "optimizer_Ground=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_Ground)\n",
    "###############\n",
    "\n",
    "#Objects channel\n",
    "Objects_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Objects_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                   stddev=0.01),name=\"Objects_weights\")\n",
    "\n",
    "Objects_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Objects_bias\")\n",
    "\n",
    "Objects_hidden=tf.nn.relu(tf.matmul(Objects_input,Objects_weights)+Objects_bias)\n",
    "\n",
    "Objects_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],\n",
    "                                    stddev=0.01),name=\"Objects_outweights\")\n",
    "\n",
    "Objects_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Objects_outbias\")\n",
    "\n",
    "Objects_out=tf.nn.sigmoid(tf.matmul(Objects_hidden,Objects_outweights)+Objects_outbias)\n",
    "\n",
    "\n",
    "loss_Objects=tf.nn.l2_loss(Objects_input-Objects_out)\n",
    "\n",
    "regularization_Objects=(tf.nn.l2_loss(Objects_weights)+tf.nn.l2_loss(Objects_bias)\n",
    "               +tf.nn.l2_loss(Objects_outweights)+tf.nn.l2_loss(Objects_outbias))\n",
    "\n",
    "loss_Objects=loss_Objects+1e-5*regularization_Objects\n",
    "\n",
    "optimizer_Objects=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_Objects)\n",
    "##########\n",
    "###Building channels\n",
    "\n",
    "Building_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Building_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                   stddev=0.01),name=\"Building_weights\")\n",
    "\n",
    "Building_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Building_bias\")\n",
    "\n",
    "Building_hidden=tf.nn.relu(tf.matmul(Building_input,Building_weights)+Building_bias)\n",
    "\n",
    "Building_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01)\n",
    "                                   ,name=\"Building_outweights\")\n",
    "\n",
    "Building_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Building_outbias\")\n",
    "\n",
    "Building_out=tf.nn.sigmoid(tf.matmul(Building_hidden,Building_outweights)+Building_outbias)\n",
    "\n",
    "loss_Building=tf.nn.l2_loss(Building_input-Building_out)\n",
    "\n",
    "regularization_Building=(tf.nn.l2_loss(Building_weights)+tf.nn.l2_loss(Building_bias)\n",
    "               +tf.nn.l2_loss(Building_outweights)+tf.nn.l2_loss(Building_outbias))\n",
    "\n",
    "loss_Building=loss_Building+1e-5*regularization_Building\n",
    "\n",
    "optimizer_Building=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_Building)\n",
    "###########\n",
    "#Sky channels\n",
    "\n",
    "Sky_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Sky_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                   stddev=0.01),name=\"Sky_weights\")\n",
    "\n",
    "Sky_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Sky_bias\")\n",
    "\n",
    "\n",
    "Sky_hidden=tf.nn.relu(tf.matmul(Sky_input,Sky_weights)+Sky_bias)\n",
    "\n",
    "Sky_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01)\n",
    "                                   ,name=\"Sky_outweights\")\n",
    "Sky_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Sky_outbias\")\n",
    "\n",
    "Sky_out=tf.nn.sigmoid(tf.matmul(Sky_hidden,Sky_outweights)+Sky_outbias)\n",
    "\n",
    "loss_Sky=tf.nn.l2_loss(Sky_input-Sky_out)\n",
    "regularization_Sky=(tf.nn.l2_loss(Sky_weights)+tf.nn.l2_loss(Sky_bias)\n",
    "               +tf.nn.l2_loss(Sky_outweights)+tf.nn.l2_loss(Sky_outbias))\n",
    "\n",
    "loss_Sky=loss_Sky+1e-5*regularization_Sky\n",
    "\n",
    "optimizer_Sky=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_Sky)\n",
    "#######\n",
    "# channel Vegetation\n",
    "                                                                                      \n",
    "Vegetation_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "Vegetation_weights=tf.Variable(tf.random_normal(shape=[1080,hidden_size],\n",
    "                                   stddev=0.01),name=\"Vegetation_weights\")\n",
    "\n",
    "Vegetation_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Vegetation_bias\")\n",
    "Vegetation_hidden=tf.nn.relu(tf.matmul(Vegetation_input,Vegetation_weights)+Vegetation_bias)\n",
    "\n",
    "Vegetation_outweights=tf.Variable(tf.random_normal(shape=[hidden_size,1080],stddev=0.01)\n",
    "                                   ,name=\"Vegetation_outweights\")\n",
    "\n",
    "Vegetation_outbias=tf.Variable(tf.zeros([1,1080]),name=\"Vegetation_outbias\")\n",
    "\n",
    "Vegetation_out=tf.nn.sigmoid(tf.matmul(Vegetation_hidden,Vegetation_outweights)+Vegetation_outbias)\n",
    "loss_Vegetation=tf.nn.l2_loss(Vegetation_input-Vegetation_out)\n",
    "\n",
    "regularization_Vegetation=(tf.nn.l2_loss(Vegetation_weights)+tf.nn.l2_loss(Vegetation_bias)\n",
    "               +tf.nn.l2_loss(Vegetation_outweights)+tf.nn.l2_loss(Vegetation_outbias))\n",
    "\n",
    "loss_Vegetation=loss_Vegetation+1e-5*regularization_Vegetation\n",
    "\n",
    "optimizer_Vegetation=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_Vegetation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  train seperate semantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "train_size=Ground_data.shape[0]\n",
    "train_indices=range(train_size)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "\n",
    "par_path=\"../par/\"\n",
    "if not os.path.isdir(par_path):\n",
    "    os.mkdir(par_path)\n",
    "\n",
    "model_path=\"../model\"\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        perm_indices=np.random.permutation(train_indices)\n",
    "        \n",
    "        for step in range(int(train_size/batch_size)):\n",
    "            \n",
    "            \n",
    "            offset=(step*batch_size)%(train_size-batch_size)\n",
    "            batch_indices=perm_indices[offset:(offset+batch_size)]\n",
    "            \n",
    "    \n",
    "            feed_dict={Ground_input:Ground_data[batch_indices,:],\n",
    "                       Objects_input:Objects_data[batch_indices,:],\n",
    "                       Vegetation_input:Vegetation_data[batch_indices,:],\n",
    "                       Building_input:Building_data[batch_indices,:],\n",
    "                       Sky_input:Sky_data[batch_indices,:]\n",
    "                      }   \n",
    "            \n",
    "     \n",
    "            \n",
    "            _Ground,l_Ground=sess.run([optimizer_Ground,loss_Ground],feed_dict=feed_dict)\n",
    "            \n",
    "            _Objects,l_Objects=sess.run([optimizer_Objects,loss_Objects],feed_dict=feed_dict)\n",
    "            \n",
    "            _Vegetation,l_Vegetation=sess.run([optimizer_Vegetation,loss_Vegetation],feed_dict=feed_dict)\n",
    "            \n",
    "            _Building,l_Building=sess.run([optimizer_Building,loss_Building],feed_dict=feed_dict)\n",
    "            \n",
    "            _Sky,l_Sky=sess.run([optimizer_Sky,loss_Sky],feed_dict=feed_dict)\n",
    "     \n",
    "    \n",
    "    saver.save(sess,model_path+\"/pretraining_sem_sep.ckpt\")\n",
    "            \n",
    "    np.save(par_path+\"Ground_weights\",sess.run(Ground_weights))\n",
    "    np.save(par_path+\"Ground_bias\",sess.run(Ground_bias))\n",
    "    np.save(par_path+\"Ground_outweights\",sess.run(Ground_outweights))\n",
    "    np.save(par_path+\"Ground_outbias\",sess.run(Ground_outbias))\n",
    "    \n",
    "    np.save(par_path+\"Objects_weights\",sess.run(Objects_weights))\n",
    "    np.save(par_path+\"Objects_bias\",sess.run(Objects_bias))\n",
    "    np.save(par_path+\"Objects_outweights\",sess.run(Objects_outweights))\n",
    "    np.save(par_path+\"Objects_outbias\",sess.run(Objects_outbias))\n",
    "    \n",
    "    np.save(par_path+\"Building_weights\",sess.run(Building_weights))\n",
    "    np.save(par_path+\"Building_bias\",sess.run(Building_bias))\n",
    "    np.save(par_path+\"Building_outweights\",sess.run(Building_outweights))\n",
    "    np.save(par_path+\"Building_outbias\",sess.run(Building_outbias))\n",
    "    \n",
    "    np.save(par_path+\"Vegetation_weights\",sess.run(Vegetation_weights))\n",
    "    np.save(par_path+\"Vegetation_bias\",sess.run(Vegetation_bias))\n",
    "    np.save(par_path+\"Vegetation_outweights\",sess.run(Vegetation_outweights))\n",
    "    np.save(par_path+\"Vegetation_outbias\",sess.run(Vegetation_outbias)) \n",
    "    \n",
    "    np.save(par_path+\"Sky_weights\",sess.run(Sky_weights))\n",
    "    np.save(par_path+\"Sky_bias\",sess.run(Sky_bias))\n",
    "    np.save(par_path+\"Sky_outweights\",sess.run(Sky_outweights))\n",
    "    np.save(par_path+\"Sky_outbias\",sess.run(Sky_outbias)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  validate semantic channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/pretraining_sem_sep.ckpt\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.0025101   0.00277153  0.00114891  0.00238199  0.01032499  0.02103932\n",
      "  0.04050805  0.11177267  0.31455237  0.77097887  0.95317197  0.98623383\n",
      "  0.9767943   0.94510728  0.81975383  0.59571815  0.36712095  0.28136688\n",
      "  0.00186928  0.00467551]\n"
     ]
    }
   ],
   "source": [
    "data_val=process_data('validation')\n",
    "Red_data_val=data_val['Red']\n",
    "Green_data_val=data_val['Green']\n",
    "Blue_data_val=data_val['Blue']\n",
    "Depth_data_val=data_val['Depth']\n",
    "Depthmask_data_val=data_val['Depthmask']\n",
    "Ground_data_val=data_val['Ground']\n",
    "Objects_data_val=data_val['Objects']\n",
    "Building_data_val=data_val['Building']\n",
    "Vegetation_data_val=data_val['Vegetation']\n",
    "Sky_data_val=data_val['Sky']\n",
    "    \n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    saver.restore(sess,\"../model/pretraining_sem_sep.ckpt\")\n",
    "    \n",
    "    for i in range(0,1):\n",
    "        \n",
    "        feed_dict={Ground_input:Ground_data_val[i:i+1,:],\n",
    "                   Objects_input:Objects_data_val[i:i+1,:],\n",
    "                   Building_input:Building_data_val[i:i+1,:],\n",
    "                   Sky_input:Vegetation_data_val[i:i+1,:],\n",
    "                   Vegetation_input:Sky_data_val[i:i+1,:],\n",
    "                  }\n",
    "        print(Objects_data_val[i:i+1,:][0][0:20])\n",
    "        print(sess.run(Objects_out,feed_dict=feed_dict)[0][0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  full  semantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  load  parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_Ground_weights=np.load(\"../par/Ground_weights.npy\")\n",
    "pre_Ground_bias=np.load(\"../par/Ground_bias.npy\")\n",
    "pre_Ground_outweights=np.load(\"../par/Ground_outweights.npy\")\n",
    "pre_Ground_outbias=np.load(\"../par/Ground_outbias.npy\")\n",
    "\n",
    "pre_Objects_weights=np.load(\"../par/Objects_weights.npy\")\n",
    "pre_Objects_bias=np.load(\"../par/Objects_bias.npy\")\n",
    "pre_Objects_outweights=np.load(\"../par/Objects_outweights.npy\")\n",
    "pre_Objects_outbias=np.load(\"../par/Objects_outbias.npy\")\n",
    "\n",
    "pre_Building_weights=np.load(\"../par/Building_weights.npy\")\n",
    "pre_Building_bias=np.load(\"../par/Building_bias.npy\")\n",
    "pre_Building_outweights=np.load(\"../par/Building_outweights.npy\")\n",
    "pre_Building_outbias=np.load(\"../par/Building_outbias.npy\")\n",
    "\n",
    "pre_Vegetation_weights=np.load(\"../par/Vegetation_weights.npy\")\n",
    "pre_Vegetation_bias=np.load(\"../par/Vegetation_bias.npy\")\n",
    "pre_Vegetation_outweights=np.load(\"../par/Vegetation_outweights.npy\")\n",
    "pre_Vegetation_outbias=np.load(\"../par/Vegetation_outbias.npy\")\n",
    "\n",
    "pre_Sky_weights=np.load(\"../par/Sky_weights.npy\")\n",
    "pre_Sky_bias=np.load(\"../par/Sky_bias.npy\")\n",
    "pre_Sky_outweights=np.load(\"../par/Sky_outweights.npy\")\n",
    "pre_Sky_outbias=np.load(\"../par/Sky_outbias.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  shared semantic model \n",
    "Ground_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "Objects_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "Building_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "Vegetation_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "Sky_input=tf.placeholder(tf.float32,shape=[None,1080])\n",
    "\n",
    "\n",
    "Ground_weights=tf.Variable(pre_Ground_weights,name=\"Ground_weights\")\n",
    "Ground_bias=tf.Variable(pre_Ground_bias,name=\"Ground_bias\")\n",
    "\n",
    "\n",
    "Objects_weights=tf.Variable(pre_Objects_weights,name=\"Objects_weights\")\n",
    "Objects_bias=tf.Variable(pre_Objects_bias,name=\"Objects_bias\")\n",
    "\n",
    "\n",
    "Building_weights=tf.Variable(pre_Building_weights,name=\"Building_weights\")\n",
    "Building_bias=tf.Variable(pre_Building_bias,name=\"Building_bias\")\n",
    "\n",
    "\n",
    "Vegetation_weights=tf.Variable(pre_Vegetation_weights,name=\"Vegetation_weights\")\n",
    "Vegetation_bias=tf.Variable(pre_Vegetation_bias,name=\"Vegetation_bias\")\n",
    "\n",
    "\n",
    "\n",
    "Sky_weights=tf.Variable(pre_Sky_weights,name=\"Sky_weights\")\n",
    "Sky_bias=tf.Variable(pre_Sky_bias,name=\"Sky_bias\")\n",
    "\n",
    "\n",
    "\n",
    "Depth_hidden=tf.nn.relu(tf.matmul(Depth_input,Depth_weights)+Depth_bias)\n",
    "Red_hidden=tf.nn.relu(tf.matmul(Red_input,Red_weights)+Red_bias)\n",
    "Blue_hidden=tf.nn.relu(tf.matmul(Blue_input,Blue_weights)+Blue_bias)\n",
    "Green_hidden=tf.nn.relu(tf.matmul(Green_input,Green_weights)+Green_bias)\n",
    "Ground_hidden=tf.nn.relu(tf.matmul(Ground_input,Ground_weights)+Ground_bias)\n",
    "Objects_hidden=tf.nn.relu(tf.matmul(Objects_input,Objects_weights)+Objects_bias)\n",
    "Building_hidden=tf.nn.relu(tf.matmul(Building_input,Building_weights)+Building_bias)\n",
    "Vegetation_hidden=tf.nn.relu(tf.matmul(Vegetation_input,Vegetation_weights)+Vegetation_bias)\n",
    "Sky_hidden=tf.nn.relu(tf.matmul(Sky_input,Sky_weights)+Sky_bias)\n",
    "\n",
    "\n",
    "\n",
    "Semantic_weights=tf.Variable(tf.random_normal(shape=[5*hidden_size,hidden_size],\n",
    "                             stddev=0.01),name=\"Semantic_weights\")\n",
    "Semantic_bias=tf.Variable(tf.zeros([1,hidden_size]),name=\"Semantic_bias\")\n",
    "\n",
    "Semantic_shared=tf.matmul(tf.concat([Ground_hidden,Objects_hidden,Building_hidden,Vegetation_hidden,Sky_hidden],1)\n",
    "                                     ,Semantic_weights)+Semantic_bias\n",
    "\n",
    "\n",
    "\n",
    "decoder_semweights=tf.Variable(tf.random_normal(shape=[hidden_size,5*hidden_size],stddev=0.01),name=\"decoder_semweights\")\n",
    "decoder_sembias=tf.Variable(tf.zeros([1,5*hidden_size]),name=\"decoder_sembias\")  \n",
    "decoder_sem=tf.matmul(Semantic_shared,decoder_semweights)+decoder_sembias\n",
    "\n",
    "decoder_ground,decoder_objects,decoder_building,decoder_vegetation,decoder_sky=tf.split(decoder_sem,num_or_size_splits=5, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Ground_outweights=tf.Variable(pre_Ground_outweights,name=\"Ground_outweights\")\n",
    "Ground_outbias=tf.Variable(pre_Ground_outbias,name=\"Ground_outbias\")\n",
    "Ground_out=tf.nn.sigmoid(tf.matmul(decoder_ground,Ground_outweights)+Ground_outbias)\n",
    "\n",
    "\n",
    "Objects_outweights=tf.Variable(pre_Objects_outweights,name=\"Objects_outweights\")\n",
    "Objects_outbias=tf.Variable(pre_Objects_outbias,name=\"Objects_outbias\")\n",
    "Objects_out=tf.nn.sigmoid(tf.matmul(decoder_objects,Objects_outweights)+Objects_outbias)\n",
    "\n",
    "\n",
    "\n",
    "Building_outweights=tf.Variable(pre_Building_outweights,name=\"Building_outweights\")\n",
    "Building_outbias=tf.Variable(pre_Building_outbias,name=\"Building_outbias\")\n",
    "Building_out=tf.nn.sigmoid(tf.matmul(decoder_building,Building_outweights)+Building_outbias)\n",
    "\n",
    "\n",
    "Vegetation_outweights=tf.Variable(pre_Vegetation_outweights,name=\"Vegetation_outweights\")\n",
    "Vegetation_outbias=tf.Variable(pre_Vegetation_outbias,name=\"Vegetation_outbias\")\n",
    "Vegetation_out=tf.nn.sigmoid(tf.matmul(decoder_vegetation,Vegetation_outweights)+Vegetation_outbias)\n",
    "\n",
    "\n",
    "Sky_outweights=tf.Variable(pre_Sky_outweights,name=\"Sky_outweights\")\n",
    "Sky_outbias=tf.Variable(pre_Sky_outbias,name=\"Sky_outbias\")\n",
    "Sky_out=tf.nn.sigmoid(tf.matmul(decoder_sky,Sky_outweights)+Sky_outbias)\n",
    "\n",
    "\n",
    "\n",
    "loss_sem=(tf.nn.l2_loss(Ground_input-Ground_out)+tf.nn.l2_loss(Objects_input-Objects_out)+\n",
    "         tf.nn.l2_loss(Building_input-Building_out)+tf.nn.l2_loss(Vegetation_input-Vegetation_out)\n",
    "         +tf.nn.l2_loss(Sky_input-Sky_out))\n",
    "\n",
    "\n",
    "\n",
    "regularization_sem=(tf.nn.l2_loss(Ground_weights)+tf.nn.l2_loss(Ground_bias)\n",
    "               +tf.nn.l2_loss(Objects_weights)+tf.nn.l2_loss(Objects_bias)\n",
    "               +tf.nn.l2_loss(Building_weights)+tf.nn.l2_loss(Building_bias)\n",
    "               +tf.nn.l2_loss(Vegetation_weights)+tf.nn.l2_loss(Vegetation_bias)\n",
    "               +tf.nn.l2_loss(Sky_weights)+tf.nn.l2_loss(Sky_bias)\n",
    "               +tf.nn.l2_loss(Semantic_weights)+tf.nn.l2_loss(Semantic_bias)\n",
    "               +tf.nn.l2_loss(decoder_semweights)+tf.nn.l2_loss(decoder_sembias)\n",
    "               +tf.nn.l2_loss(Ground_outweights)+tf.nn.l2_loss(Ground_outbias)\n",
    "               +tf.nn.l2_loss(Objects_outweights)+tf.nn.l2_loss(Objects_outbias)\n",
    "               +tf.nn.l2_loss(Building_outweights)+tf.nn.l2_loss(Building_outbias)\n",
    "               +tf.nn.l2_loss(Vegetation_outweights)+tf.nn.l2_loss(Vegetation_outbias)\n",
    "               +tf.nn.l2_loss(Sky_outweights)+tf.nn.l2_loss(Sky_outbias)\n",
    "               )\n",
    "loss_sem=loss_sem+1e-5*regularization_sem\n",
    "\n",
    "optimizer_sem=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss_sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  train the shared sem channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/pretraining.ckpt\n"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "train_size=Ground_data_val.shape[0]\n",
    "train_indices=range(train_size)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    \n",
    "    saver.restore(sess,\"../model/pretraining.ckpt\")\n",
    "    \n",
    "    for i in range(0,1):#len(Red_data_val)):\n",
    "        \n",
    "        feed_dict={Red_input:Red_data_val[i:i+1,:],\n",
    "                   Green_input:Green_data_val[i:i+1,:],\n",
    "                   Blue_input:Blue_data_val[i:i+1,:]\n",
    "                  }\n",
    "        \n",
    "#        outred_val=(np.reshape(sess.run(Red_out,feed_dict=feed_dict),(60,18)).transpose()*255).astype(np.uint8)\n",
    "#        inred=(np.reshape(Red_data_val[i:i+1,:],(60,18)).transpose()*255).astype(np.uint8)\n",
    "    \n",
    "#        outblue_val=(np.reshape(sess.run(Blue_out,feed_dict=feed_dict),(60,18)).transpose()*255).astype(np.uint8)\n",
    "#        inblue=(np.reshape(Red_data_val[i:i+1,:],(60,18)).transpose()*255).astype(np.uint8)\n",
    "    \n",
    "#        outgreen_val=(np.reshape(sess.run(Green_out,feed_dict=feed_dict),(60,18)).transpose()*255).astype(np.uint8)\n",
    "#        ingreen=(np.reshape(Green_data_val[i:i+1,:],(60,18)).transpose()*255).astype(np.uint8)\n",
    "        \n",
    "    \n",
    "        \n",
    "#        inimg=np.dstack((inred,ingreen,inblue))\n",
    "#        im = Image.fromarray(inimg)\n",
    "#        im.save(\"../images/inval_%04d.png\"%i)\n",
    "        \n",
    "        \n",
    "#        outimg=np.dstack((outred_val,outgreen_val,outblue_val))\n",
    "#        om = Image.fromarray(outimg)\n",
    "#        om.save(\"../images/outval_%04d.png\"%i)\n",
    "# to convert to a video\n",
    "#ffmpeg -r 25 -i inval_%04d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p in.mp4\n",
    "#ffmpeg -r 25 -i outval_%04d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p out.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
