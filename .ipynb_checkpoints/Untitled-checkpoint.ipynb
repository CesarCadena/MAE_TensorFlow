{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load PretrainingMAE.py\n",
    "\n",
    "    def AE_red(self,input):\n",
    "\n",
    "        self.red_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='red_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"red_ec_layer_bias\")}\n",
    "        self.layers.append([self.red_ec_layer])\n",
    "\n",
    "        hidden_red = tf.nn.relu(tf.add(tf.matmul(input, self.red_ec_layer['weights']),self.red_ec_layer['bias']))\n",
    "\n",
    "        self.red_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='red_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"red_dc_layer_bias\")}\n",
    "        self.layers.append(self.red_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_red, self.red_dc_layer['weights']),self.red_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_green(self,input):\n",
    "\n",
    "        self.green_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='green_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"green_ec_layer_bias\")}\n",
    "        self.layers.append([self.green_ec_layer])\n",
    "\n",
    "        hidden_green = tf.nn.relu(tf.add(tf.matmul(input, self.green_ec_layer['weights']),self.green_ec_layer['bias']))\n",
    "\n",
    "        self.green_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='green_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"green_dc_layer_bias\")}\n",
    "        self.layers.append(self.green_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_green, self.green_dc_layer['weights']),self.green_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_blue(self,input):\n",
    "\n",
    "        self.blue_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='blue_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"blue_ec_layer_bias\")}\n",
    "        self.layers.append([self.blue_ec_layer])\n",
    "\n",
    "        hidden_blue = tf.nn.relu(tf.add(tf.matmul(input, self.blue_ec_layer['weights']),self.blue_ec_layer['bias']))\n",
    "\n",
    "        self.blue_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='blue_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"blue_dc_layer_bias\")}\n",
    "        self.layers.append(self.blue_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_blue, self.blue_dc_layer['weights']),self.blue_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_depth(self,input):\n",
    "\n",
    "        self.depth_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='depth_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"depth_ec_layer_bias\")}\n",
    "        self.layers.append([self.depth_ec_layer])\n",
    "\n",
    "        hidden_depth = tf.nn.relu(tf.add(tf.matmul(input, self.depth_ec_layer['weights']),self.depth_ec_layer['bias']))\n",
    "\n",
    "        self.depth_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='depth_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"depth_dc_layer_bias\")}\n",
    "        self.layers.append(self.depth_dc_layer)\n",
    "\n",
    "        output = tf.add(tf.matmul(hidden_depth, self.depth_dc_layer['weights']),self.depth_dc_layer['bias'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_gnd(self,input):\n",
    "\n",
    "        self.gnd_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='gnd_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"gnd_ec_layer_bias\")}\n",
    "        self.layers.append([self.gnd_ec_layer])\n",
    "\n",
    "        hidden_gnd = tf.nn.relu(tf.add(tf.matmul(input, self.gnd_ec_layer['weights']),self.gnd_ec_layer['bias']))\n",
    "\n",
    "        self.gnd_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='gnd_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"gnd_dc_layer_bias\")}\n",
    "        self.layers.append(self.gnd_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_gnd, self.gnd_dc_layer['weights']),self.gnd_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_obj(self,input):\n",
    "\n",
    "        self.obj_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='obj_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"obj_ec_layer_bias\")}\n",
    "        self.layers.append([self.obj_ec_layer])\n",
    "\n",
    "        hidden_obj = tf.nn.relu(tf.add(tf.matmul(input, self.obj_ec_layer['weights']),self.obj_ec_layer['bias']))\n",
    "\n",
    "        self.obj_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='obj_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"obj_dc_layer_bias\")}\n",
    "        self.layers.append(self.obj_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_obj, self.obj_dc_layer['weights']),self.obj_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_bld(self,input):\n",
    "\n",
    "        self.bld_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='bld_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"bld_ec_layer_bias\")}\n",
    "        self.layers.append([self.bld_ec_layer])\n",
    "\n",
    "        hidden_bld = tf.nn.relu(tf.add(tf.matmul(input, self.bld_ec_layer['weights']),self.bld_ec_layer['bias']))\n",
    "\n",
    "        self.bld_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='bld_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"bld_dc_layer_bias\")}\n",
    "        self.layers.append(self.bld_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_bld, self.bld_dc_layer['weights']),self.bld_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_veg(self,input):\n",
    "\n",
    "        self.veg_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='veg_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"veg_ec_layer_bias\")}\n",
    "        self.layers.append([self.veg_ec_layer])\n",
    "\n",
    "        hidden_veg = tf.nn.relu(tf.add(tf.matmul(input, self.veg_ec_layer['weights']),self.veg_ec_layer['bias']))\n",
    "\n",
    "        self.veg_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='veg_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"veg_dc_layer_bias\")}\n",
    "        self.layers.append(self.veg_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_veg, self.veg_dc_layer['weights']),self.veg_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_sky(self,input):\n",
    "\n",
    "        self.sky_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.input_size, self.hidden_size], stddev=0.01), name='sky_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"sky_ec_layer_bias\")}\n",
    "        self.layers.append([self.sky_ec_layer])\n",
    "\n",
    "        hidden_sky = tf.nn.relu(tf.add(tf.matmul(input, self.sky_ec_layer['weights']),self.sky_ec_layer['bias']))\n",
    "\n",
    "        self.sky_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, self.input_size], stddev=0.01), name='sky_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.input_size]), name=\"sky_dc_layer_bias\")}\n",
    "\n",
    "        self.layers.append(self.sky_dc_layer)\n",
    "\n",
    "        output = tf.nn.sigmoid(tf.add(tf.matmul(hidden_sky, self.sky_dc_layer['weights']),self.sky_dc_layer['bias']))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def AE_sem(self,gnd,obj,bld,veg,sky):\n",
    "\n",
    "        hidden_gnd = tf.nn.relu(tf.add(tf.matmul(gnd,self.gnd_ec_layer['weights']),self.gnd_ec_layer['bias']))\n",
    "        hidden_obj = tf.nn.relu(tf.add(tf.matmul(obj,self.obj_ec_layer['weights']),self.obj_ec_layer['bias']))\n",
    "        hidden_bld = tf.nn.relu(tf.add(tf.matmul(bld,self.bld_ec_layer['weights']),self.bld_ec_layer['bias']))\n",
    "        hidden_veg = tf.nn.relu(tf.add(tf.matmul(veg,self.veg_ec_layer['weights']),self.veg_ec_layer['bias']))\n",
    "        hidden_sky = tf.nn.relu(tf.add(tf.matmul(sky,self.sky_ec_layer['weights']),self.sky_ec_layer['bias']))\n",
    "\n",
    "        sem = tf.concat([hidden_gnd,hidden_obj,hidden_bld,hidden_veg,hidden_sky],axis=1)\n",
    "\n",
    "        self.sem_ec_layer = {'weights':tf.Variable(tf.random_normal(shape=[5*self.hidden_size, self.hidden_size], stddev=0.01), name='sem_ec_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([self.hidden_size]), name=\"sem_ec_layer_bias\")}\n",
    "        self.layers.append(self.sem_ec_layer)\n",
    "\n",
    "        hidden_sem = tf.nn.relu(tf.add(tf.matmul(sem,self.sem_ec_layer['weights']),self.sem_ec_layer['bias']))\n",
    "\n",
    "        self.sem_dc_layer = {'weights':tf.Variable(tf.random_normal(shape=[self.hidden_size, 5*self.hidden_size], stddev=0.01), name='sem_dc_layer_weights'),\n",
    "                             'bias':tf.Variable(tf.zeros([5*self.hidden_size]), name=\"sem_dc_layer_bias\")}\n",
    "        self.layers.append(self.sem_dc_layer)\n",
    "\n",
    "        sem_out = tf.nn.relu(tf.add(tf.matmul(hidden_sem,self.sem_dc_layer['weights']),self.sem_dc_layer['bias']))\n",
    "\n",
    "        hidden_gnd_out,hidden_obj_out,hidden_bld_out,hidden_veg_out,hidden_sky_out = tf.split(sem_out,5,axis=1)\n",
    "\n",
    "\n",
    "        gnd_output = tf.sigmoid(tf.add(tf.matmul(hidden_gnd_out,self.gnd_dc_layer['weights']),self.gnd_dc_layer['bias']))\n",
    "        obj_output = tf.sigmoid(tf.add(tf.matmul(hidden_obj_out,self.obj_dc_layer['weights']),self.obj_dc_layer['bias']))\n",
    "        bld_output = tf.sigmoid(tf.add(tf.matmul(hidden_bld_out,self.bld_dc_layer['weights']),self.bld_dc_layer['bias']))\n",
    "        veg_output = tf.sigmoid(tf.add(tf.matmul(hidden_veg_out,self.veg_dc_layer['weights']),self.veg_dc_layer['bias']))\n",
    "        sky_output = tf.sigmoid(tf.add(tf.matmul(hidden_sky_out,self.sky_dc_layer['weights']),self.sky_dc_layer['bias']))\n",
    "\n",
    "        return gnd_output,obj_output,bld_output,veg_output,sky_output\n",
    "\n",
    "    def pretrain_seperate_channels(self):\n",
    "\n",
    "        red_pred = self.AE_red(self.input_red)\n",
    "        green_pred = self.AE_green(self.input_green)\n",
    "        blue_pred = self.AE_blue(self.input_blue)\n",
    "        depth_pred = self.AE_depth(self.input_depth)\n",
    "        gnd_pred = self.AE_gnd(self.input_gnd)\n",
    "        obj_pred = self.AE_obj(self.input_obj)\n",
    "        bld_pred = self.AE_bld(self.input_bld)\n",
    "        veg_pred = self.AE_veg(self.input_veg)\n",
    "        sky_pred = self.AE_sky(self.input_sky)\n",
    "\n",
    "        cost_red = tf.nn.l2_loss(red_pred-self.label_red)\n",
    "        cost_green = tf.nn.l2_loss(green_pred-self.label_green)\n",
    "        cost_blue = tf.nn.l2_loss(blue_pred-self.label_blue)\n",
    "        cost_depth = tf.nn.l2_loss(tf.multiply(self.depth_mask,depth_pred)-tf.multiply(self.depth_mask,self.label_depth))\n",
    "        cost_gnd = tf.nn.l2_loss(gnd_pred-self.label_gnd)\n",
    "        cost_obj = tf.nn.l2_loss(obj_pred-self.label_obj)\n",
    "        cost_bld = tf.nn.l2_loss(bld_pred-self.label_bld)\n",
    "        cost_veg = tf.nn.l2_loss(veg_pred-self.label_veg)\n",
    "        cost_sky = tf.nn.l2_loss(sky_pred-self.label_sky)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "\n",
    "        reg_red = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.red_ec_layer['weights'],\n",
    "                                                                             self.red_dc_layer['weights'],\n",
    "                                                                             self.red_ec_layer['bias'],\n",
    "                                                                             self.red_dc_layer['bias']])\n",
    "\n",
    "        reg_green = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.green_ec_layer['weights'],\n",
    "                                                                             self.green_dc_layer['weights'],\n",
    "                                                                             self.green_ec_layer['bias'],\n",
    "                                                                             self.green_dc_layer['bias']])\n",
    "\n",
    "        reg_blue = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.blue_ec_layer['weights'],\n",
    "                                                                             self.blue_dc_layer['weights'],\n",
    "                                                                             self.blue_ec_layer['bias'],\n",
    "                                                                             self.blue_dc_layer['bias']])\n",
    "\n",
    "        reg_depth = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.depth_ec_layer['weights'],\n",
    "                                                                             self.depth_dc_layer['weights'],\n",
    "                                                                             self.depth_ec_layer['bias'],\n",
    "                                                                             self.depth_dc_layer['bias']])\n",
    "\n",
    "        reg_gnd = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.gnd_ec_layer['weights'],\n",
    "                                                                             self.gnd_dc_layer['weights'],\n",
    "                                                                             self.gnd_ec_layer['bias'],\n",
    "                                                                             self.gnd_dc_layer['bias']])\n",
    "\n",
    "        reg_obj = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.obj_ec_layer['weights'],\n",
    "                                                                             self.obj_dc_layer['weights'],\n",
    "                                                                             self.obj_ec_layer['bias'],\n",
    "                                                                             self.obj_dc_layer['bias']])\n",
    "\n",
    "        reg_bld = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.bld_ec_layer['weights'],\n",
    "                                                                             self.bld_dc_layer['weights'],\n",
    "                                                                             self.bld_ec_layer['bias'],\n",
    "                                                                             self.bld_dc_layer['bias']])\n",
    "\n",
    "        reg_veg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.veg_ec_layer['weights'],\n",
    "                                                                             self.veg_dc_layer['weights'],\n",
    "                                                                             self.veg_ec_layer['bias'],\n",
    "                                                                             self.veg_dc_layer['bias']])\n",
    "\n",
    "        reg_sky = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.sky_ec_layer['weights'],\n",
    "                                                                             self.sky_dc_layer['weights'],\n",
    "                                                                             self.sky_ec_layer['bias'],\n",
    "                                                                             self.sky_dc_layer['bias']])\n",
    "\n",
    "        cost_red += reg_red\n",
    "        cost_green += reg_green\n",
    "        cost_blue += reg_blue\n",
    "        cost_depth += reg_depth\n",
    "        cost_gnd += reg_gnd\n",
    "        cost_obj += reg_obj\n",
    "        cost_bld += reg_bld\n",
    "        cost_veg += reg_veg\n",
    "        cost_sky += reg_sky\n",
    "\n",
    "\n",
    "        summary_red = tf.summary.scalar('cost_red',cost_red)\n",
    "        summary_green = tf.summary.scalar('cost_green',cost_green)\n",
    "        summary_blue = tf.summary.scalar('cost_blue',cost_blue)\n",
    "        summary_depth = tf.summary.scalar('cost_depth',cost_depth)\n",
    "        summary_gnd = tf.summary.scalar('cost_gnd',cost_gnd)\n",
    "        summary_obj = tf.summary.scalar('cost_obj',cost_obj)\n",
    "        summary_bld = tf.summary.scalar('cost_bld',cost_bld)\n",
    "        summary_veg = tf.summary.scalar('cost_veg',cost_veg)\n",
    "        summary_sky = tf.summary.scalar('cost_sky',cost_sky)\n",
    "\n",
    "        opt_red = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_red)\n",
    "        opt_green = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_green)\n",
    "        opt_blue = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_blue)\n",
    "        opt_depth = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_depth)\n",
    "        opt_gnd = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_gnd)\n",
    "        opt_obj = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_obj)\n",
    "        opt_bld = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_bld)\n",
    "        opt_veg = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_veg)\n",
    "        opt_sky = tf.train.AdamOptimizer(learning_rate=0.000001).minimize(cost_sky)\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.imr_train)/self.batch_size)\n",
    "            epoch_losses = []\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                epoch_loss = np.zeros((9,1))\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    imr_batch = self.imr_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    img_batch = self.img_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    imb_batch = self.imb_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    depth_batch = self.depth_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    gnd_batch = self.gnd_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    obj_batch = self.obj_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    bld_batch = self.bld_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    veg_batch = self.veg_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    sky_batch = self.sky_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    depth_mask = self.depth_mask_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "\n",
    "                    imr_in,img_in,imb_in,depth_in,gnd_in,obj_in,bld_in,veg_in,sky_in = pretraining_input_distortion(imr_batch,\n",
    "                                                                                                                    img_batch,\n",
    "                                                                                                                    imb_batch,\n",
    "                                                                                                                    depth_batch,\n",
    "                                                                                                                    gnd_batch,\n",
    "                                                                                                                    obj_batch,\n",
    "                                                                                                                    bld_batch,\n",
    "                                                                                                                    veg_batch,\n",
    "                                                                                                                    sky_batch,\n",
    "                                                                                                                    resolution=(18,60))\n",
    "\n",
    "                    feed_dict_red = {self.input_red:imr_in,\n",
    "                                     self.label_red:imr_batch}\n",
    "\n",
    "                    feed_dict_green = {self.input_green:img_in,\n",
    "                                     self.label_green:img_batch}\n",
    "\n",
    "                    feed_dict_blue = {self.input_blue:imb_in,\n",
    "                                     self.label_blue:imb_batch}\n",
    "\n",
    "                    feed_dict_depth = {self.input_depth:depth_in,\n",
    "                                       self.depth_mask:depth_mask,\n",
    "                                       self.label_depth:depth_batch}\n",
    "\n",
    "                    feed_dict_gnd = {self.input_gnd:gnd_in,\n",
    "                                     self.label_gnd:gnd_batch}\n",
    "                    feed_dict_obj = {self.input_obj:obj_in,\n",
    "                                     self.label_obj:obj_batch}\n",
    "                    feed_dict_bld = {self.input_bld:bld_in,\n",
    "                                     self.label_bld:bld_batch}\n",
    "                    feed_dict_veg = {self.input_veg:veg_in,\n",
    "                                     self.label_veg:veg_batch}\n",
    "                    feed_dict_sky = {self.input_sky:sky_in,\n",
    "                                     self.label_sky:sky_batch}\n",
    "\n",
    "                    sum_red, _, c_red = sess.run([summary_red, opt_red, cost_red], feed_dict=feed_dict_red)\n",
    "                    epoch_loss[0] += c_red\n",
    "\n",
    "                    sum_green, _, c_green = sess.run([summary_green, opt_green, cost_green], feed_dict=feed_dict_green)\n",
    "                    epoch_loss[1] += c_green\n",
    "\n",
    "                    sum_blue, _, c_blue = sess.run([summary_blue, opt_blue, cost_blue],feed_dict=feed_dict_blue)\n",
    "                    epoch_loss[2] += c_blue\n",
    "\n",
    "                    sum_depth, _, c_depth = sess.run([summary_depth, opt_depth,cost_depth],feed_dict=feed_dict_depth)\n",
    "                    epoch_loss[3] += c_depth\n",
    "\n",
    "                    sum_gnd, _, c_gnd = sess.run([summary_gnd, opt_gnd, cost_gnd], feed_dict=feed_dict_gnd)\n",
    "                    epoch_loss[4] += c_gnd\n",
    "\n",
    "                    sum_obj, _, c_obj = sess.run([summary_obj, opt_obj, cost_obj], feed_dict=feed_dict_obj)\n",
    "                    epoch_loss[5] += c_obj\n",
    "\n",
    "                    sum_bld, _, c_bld = sess.run([summary_bld, opt_bld, cost_bld],feed_dict=feed_dict_bld)\n",
    "                    epoch_loss[6] += c_bld\n",
    "\n",
    "                    sum_veg, _, c_veg = sess.run([summary_veg, opt_veg, cost_veg], feed_dict=feed_dict_veg)\n",
    "                    epoch_loss[7] += c_veg\n",
    "\n",
    "                    sum_sky, _, c_sky = sess.run([summary_sky, opt_sky, cost_sky],feed_dict=feed_dict_sky)\n",
    "                    epoch_loss[8] += c_sky\n",
    "\n",
    "                epoch_losses.append(epoch_loss)\n",
    "\n",
    "                train_writer1.add_summary(sum_red,epoch)\n",
    "                train_writer1.add_summary(sum_green,epoch)\n",
    "                train_writer1.add_summary(sum_blue,epoch)\n",
    "                train_writer1.add_summary(sum_depth,epoch)\n",
    "                train_writer1.add_summary(sum_gnd,epoch)\n",
    "                train_writer1.add_summary(sum_obj,epoch)\n",
    "                train_writer1.add_summary(sum_bld,epoch)\n",
    "                train_writer1.add_summary(sum_veg,epoch)\n",
    "                train_writer1.add_summary(sum_sky,epoch)\n",
    "\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Loss Red Channel: ', epoch_losses[epoch][0])\n",
    "                print('Loss Green Channel: ', epoch_losses[epoch][1])\n",
    "                print('Loss Blue Channel: ', epoch_losses[epoch][2])\n",
    "                print('Loss Depth Channel: ', epoch_losses[epoch][3])\n",
    "                print('Loss Ground Channel: ', epoch_losses[epoch][4])\n",
    "                print('Loss Objects Channel: ', epoch_losses[epoch][5])\n",
    "                print('Loss Buildings Channel: ', epoch_losses[epoch][6])\n",
    "                print('Loss Vegetation Channel: ', epoch_losses[epoch][7])\n",
    "                print('Loss Sky Channel: ', epoch_losses[epoch][8])\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained1.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_red_channel(self):\n",
    "\n",
    "        red_pred = self.AE_red(self.input_red)\n",
    "        cost_red = tf.nn.l2_loss(red_pred-self.label_red)\n",
    "        loss = tf.nn.l2_loss(red_pred-self.label_red)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0005)\n",
    "        reg_red = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.red_ec_layer['weights'],\n",
    "                                                                                   self.red_dc_layer['weights'],\n",
    "                                                                                   self.red_ec_layer['bias'],\n",
    "                                                                                   self.red_dc_layer['bias']])\n",
    "        cost_red += reg_red\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Red Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Red Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt_red = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_red)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost_red)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.imr_train)/self.batch_size)\n",
    "\n",
    "            # comment for complete training on gpu\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    imr_batch = self.imr_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    imr_in = pretraining_input_distortion(copy(imr_batch))\n",
    "\n",
    "                    feed_dict = {self.input_red:imr_in,\n",
    "                                 self.label_red:imr_batch}\n",
    "\n",
    "                    _, l = sess.run([opt_red, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    imr_label = self.imr_val[i]\n",
    "                    imr_in = pretraining_input_distortion(copy(imr_label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_red:imr_in,\n",
    "                                     self.label_red:[imr_label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([red_pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_red.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_green_channel(self):\n",
    "\n",
    "        pred = self.AE_green(self.input_green)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_green)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_green)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.green_ec_layer['weights'],\n",
    "                                                                                    self.green_dc_layer['weights'],\n",
    "                                                                                   self.green_ec_layer['bias'],\n",
    "                                                                                   self.green_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Green Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Green Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.img_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    img_batch = self.img_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    img_in = pretraining_input_distortion(copy(img_batch))\n",
    "\n",
    "                    feed_dict = {self.input_green:img_in,\n",
    "                                 self.label_green:img_batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    img_label = self.img_val[i]\n",
    "                    img_in = pretraining_input_distortion(copy(img_label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_green:img_in,\n",
    "                                     self.label_green:[img_label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_green.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_blue_channel(self):\n",
    "\n",
    "        pred = self.AE_blue(self.input_blue)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_blue)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_blue)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.blue_ec_layer['weights'],\n",
    "                                                                               self.blue_dc_layer['weights'],\n",
    "                                                                               self.blue_ec_layer['bias'],\n",
    "                                                                               self.blue_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Blue Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Blue Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.imb_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    imb_batch = self.imb_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    imb_in = pretraining_input_distortion(copy(imb_batch))\n",
    "\n",
    "                    feed_dict = {self.input_blue:imb_in,\n",
    "                                 self.label_blue:imb_batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    imb_label = self.imb_val[i]\n",
    "                    imb_in = pretraining_input_distortion(copy(imb_label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_blue:imb_in,\n",
    "                                     self.label_blue:[imb_label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_blue.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_depth_channel(self):\n",
    "\n",
    "        print('Depth Pretraining')\n",
    "\n",
    "        pred = self.AE_depth(self.input_depth)\n",
    "        cost = tf.nn.l2_loss(tf.multiply(self.depth_mask,pred)-tf.multiply(self.depth_mask,self.label_depth))\n",
    "\n",
    "\n",
    "        loss = tf.nn.l2_loss(tf.multiply(self.depth_mask,pred)-tf.multiply(self.depth_mask,self.label_depth))\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.depth_ec_layer['weights'],\n",
    "                                                                               self.depth_dc_layer['weights'],\n",
    "                                                                               self.depth_ec_layer['bias'],\n",
    "                                                                               self.depth_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Depth Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Depth Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.imb_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.depth_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "                    batch_mask = self.depth_mask_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "                    batch_loss_mask = self.depth_loss_mask_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    depth_in = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_depth:depth_in,\n",
    "                                 self.label_depth:batch,\n",
    "                                 self.depth_mask:batch_mask,\n",
    "                                 self.depth_loss_mask:batch_loss_mask}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                normalization = 0\n",
    "                for i in set_val:\n",
    "                    depth_label = self.depth_val[i]\n",
    "                    depth_mask = self.depth_mask_val[i]\n",
    "                    normalization = normalization + np.count_nonzero(depth_mask)\n",
    "                    depth_loss_mask = self.depth_loss_mask_val[i]\n",
    "                    depth_in = pretraining_input_distortion(copy(depth_label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_depth:depth_in,\n",
    "                                     self.label_depth:[depth_label],\n",
    "                                     self.depth_mask:[depth_mask],\n",
    "                                     self.depth_loss_mask:[depth_loss_mask]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                # uncomment when not running on gpu\n",
    "                #if epoch%10==0:\n",
    "                     #print_validation_frames(depth_in,im_pred,depth_label,channel='depth',shape=(60,18))\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/normalization)\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_depth.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_gnd_channel(self):\n",
    "\n",
    "        pred = self.AE_gnd(self.input_gnd)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_gnd)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_gnd)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.gnd_ec_layer['weights'],\n",
    "                                                                                self.gnd_dc_layer['weights'],\n",
    "                                                                                self.gnd_ec_layer['bias'],\n",
    "                                                                                self.gnd_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Ground Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Ground Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.gnd_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.gnd_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    inp = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_gnd:inp,\n",
    "                                 self.label_gnd:batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label = self.gnd_val[i]\n",
    "                    inp = pretraining_input_distortion(copy(label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_gnd:inp,\n",
    "                                     self.label_gnd:[label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_gnd.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_obj_channel(self):\n",
    "\n",
    "        pred = self.AE_obj(self.input_obj)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_obj)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_obj)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.obj_ec_layer['weights'],\n",
    "                                                                                self.obj_dc_layer['weights'],\n",
    "                                                                                self.obj_ec_layer['bias'],\n",
    "                                                                                self.obj_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Object Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Object Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.obj_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.obj_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    inp = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_obj:inp,\n",
    "                                 self.label_obj:batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label = self.obj_val[i]\n",
    "                    inp = pretraining_input_distortion(copy(label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_obj:inp,\n",
    "                                     self.label_obj:[label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_obj.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_bld_channel(self):\n",
    "\n",
    "        pred = self.AE_bld(self.input_bld)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_bld)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_bld)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.bld_ec_layer['weights'],\n",
    "                                                                                self.bld_dc_layer['weights'],\n",
    "                                                                                self.bld_ec_layer['bias'],\n",
    "                                                                                self.bld_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Building Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Building Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.bld_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.bld_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    inp = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_bld:inp,\n",
    "                                 self.label_bld:batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label = self.bld_val[i]\n",
    "                    inp = pretraining_input_distortion(copy(label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_bld:inp,\n",
    "                                     self.label_bld:[label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_bld.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_veg_channel(self):\n",
    "\n",
    "        pred = self.AE_veg(self.input_veg)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_veg)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_veg)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.veg_ec_layer['weights'],\n",
    "                                                                                self.veg_dc_layer['weights'],\n",
    "                                                                                self.veg_ec_layer['bias'],\n",
    "                                                                                self.veg_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Vegetation Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Vegetation Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.veg_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.veg_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    inp = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_veg:inp,\n",
    "                                 self.label_veg:batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label = self.veg_val[i]\n",
    "                    inp = pretraining_input_distortion(copy(label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_veg:inp,\n",
    "                                     self.label_veg:[label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_veg.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_sky_channel(self):\n",
    "\n",
    "        pred = self.AE_sky(self.input_sky)\n",
    "        cost = tf.nn.l2_loss(pred-self.label_sky)\n",
    "        loss = tf.nn.l2_loss(pred-self.label_sky)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=1e-05)\n",
    "        reg = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.sky_ec_layer['weights'],\n",
    "                                                                                self.sky_dc_layer['weights'],\n",
    "                                                                                self.sky_ec_layer['bias'],\n",
    "                                                                                self.sky_dc_layer['bias']])\n",
    "        cost += reg\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Sky Channel',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Sky Channel',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = int(len(self.sky_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    batch = self.sky_train[_*self.batch_size:(_+1)*self.batch_size,:]\n",
    "\n",
    "                    inp = pretraining_input_distortion(copy(batch))\n",
    "\n",
    "                    feed_dict = {self.input_sky:inp,\n",
    "                                 self.label_sky:batch}\n",
    "\n",
    "                    _, l = sess.run([opt, epoch_loss_update], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "\n",
    "                print('----------------------------------------------------------------')\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label = self.sky_val[i]\n",
    "                    inp = pretraining_input_distortion(copy(label),singleframe=True)\n",
    "\n",
    "                    feed_dict_val = {self.input_sky:inp,\n",
    "                                     self.label_sky:[label]}\n",
    "\n",
    "                    im_pred,c_val = sess.run([pred,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "            if self.saving == True:\n",
    "                saver.save(sess,self.FLAGS.train_dir+'/pretrained_sky.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def pretrain_shared_semantics(self):\n",
    "\n",
    "        gnd_pred = self.AE_gnd(self.input_gnd)\n",
    "        obj_pred = self.AE_obj(self.input_obj)\n",
    "        bld_pred = self.AE_bld(self.input_bld)\n",
    "        veg_pred = self.AE_veg(self.input_veg)\n",
    "        sky_pred = self.AE_sky(self.input_sky)\n",
    "\n",
    "        gnd_pred,obj_pred,bld_pred,veg_pred,sky_pred = self.AE_sem(self.input_gnd,\n",
    "                                                                   self.input_obj,\n",
    "                                                                   self.input_bld,\n",
    "                                                                   self.input_veg,\n",
    "                                                                   self.input_sky)\n",
    "\n",
    "        cost = tf.nn.l2_loss(gnd_pred-self.label_gnd) + \\\n",
    "               tf.nn.l2_loss(obj_pred-self.label_obj) + \\\n",
    "               tf.nn.l2_loss(bld_pred-self.label_bld) + \\\n",
    "               tf.nn.l2_loss(veg_pred-self.label_veg) + \\\n",
    "               tf.nn.l2_loss(sky_pred-self.label_sky)\n",
    "\n",
    "\n",
    "        loss = tf.nn.l2_loss(gnd_pred-self.label_gnd) + \\\n",
    "               tf.nn.l2_loss(obj_pred-self.label_obj) + \\\n",
    "               tf.nn.l2_loss(bld_pred-self.label_bld) + \\\n",
    "               tf.nn.l2_loss(veg_pred-self.label_veg) + \\\n",
    "               tf.nn.l2_loss(sky_pred-self.label_sky)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "        reg_sem = tf.contrib.layers.apply_regularization(regularizer,weights_list=[self.gnd_ec_layer['weights'],\n",
    "                                                                                    self.gnd_ec_layer['bias'],\n",
    "                                                                                    self.obj_ec_layer['weights'],\n",
    "                                                                                    self.obj_ec_layer['bias'],\n",
    "                                                                                    self.bld_ec_layer['weights'],\n",
    "                                                                                    self.bld_ec_layer['bias'],\n",
    "                                                                                    self.veg_ec_layer['weights'],\n",
    "                                                                                    self.veg_ec_layer['bias'],\n",
    "                                                                                    self.sky_ec_layer['weights'],\n",
    "                                                                                    self.sky_ec_layer['bias'],\n",
    "                                                                                    self.sem_ec_layer['weights'],\n",
    "                                                                                    self.sem_ec_layer['bias'],\n",
    "                                                                                    self.sem_dc_layer['weights'],\n",
    "                                                                                    self.sem_dc_layer['bias'],\n",
    "                                                                                    self.gnd_dc_layer['weights'],\n",
    "                                                                                    self.gnd_dc_layer['bias'],\n",
    "                                                                                    self.obj_dc_layer['weights'],\n",
    "                                                                                    self.obj_dc_layer['bias'],\n",
    "                                                                                    self.bld_dc_layer['weights'],\n",
    "                                                                                    self.bld_dc_layer['bias'],\n",
    "                                                                                    self.veg_dc_layer['weights'],\n",
    "                                                                                    self.veg_dc_layer['bias'],\n",
    "                                                                                    self.sky_dc_layer['weights'],\n",
    "                                                                                    self.sky_dc_layer['bias'],])\n",
    "        cost += reg_sem\n",
    "\n",
    "        epoch_loss = tf.Variable(0.0,name='epoch_loss',trainable=False)\n",
    "        val_loss = tf.Variable(0.0,name='val_loss',trainable=False)\n",
    "\n",
    "        sum_epoch_loss = tf.summary.scalar('Epoch Loss Shared Semantics',epoch_loss)\n",
    "        sum_val_loss = tf.summary.scalar('Validation Loss Shared Semantics',val_loss)\n",
    "\n",
    "        if self.decay == 'constant':\n",
    "            learning_rate = 0.0001\n",
    "\n",
    "        if self.decay == 'piecewise':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            boundaries = [10000,100000,1000000]\n",
    "            rates = [0.001,0.0001,0.00001,0.000001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step,boundaries,rates)\n",
    "\n",
    "        if self.decay == 'exponential':\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            base_lr = 0.01\n",
    "            learning_rate = tf.train.exponential_decay(base_lr,global_step,1000,0.9)\n",
    "\n",
    "        opt1 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,var_list=[self.sem_ec_layer['weights'],\n",
    "                                                                                           self.sem_ec_layer['bias'],\n",
    "                                                                                           self.sem_dc_layer['weights'],\n",
    "                                                                                           self.sem_dc_layer['bias']])\n",
    "        opt2 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # validation objects\n",
    "        validations = np.arange(0,self.n_validation_data)\n",
    "        set_val = np.random.choice(validations,self.n_training_validations,replace=False)\n",
    "\n",
    "        epoch_loss_reset = epoch_loss.assign(0)\n",
    "        epoch_loss_update = epoch_loss.assign_add(cost)\n",
    "\n",
    "        loss_val_reset = val_loss.assign(0)\n",
    "        loss_val_update = val_loss.assign_add(loss/1080.)\n",
    "\n",
    "\n",
    "        config = tf.ConfigProto(log_device_placement=False)\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "\n",
    "\n",
    "        saver_load_gnd = tf.train.Saver({'gnd_ec_layer_weights':self.gnd_ec_layer['weights'],\n",
    "                                         'gnd_ec_layer_bias':self.gnd_ec_layer['bias'],\n",
    "                                         'gnd_dc_layer_weights':self.gnd_dc_layer['weights'],\n",
    "                                         'gnd_dc_layer_bias':self.gnd_dc_layer['bias']})\n",
    "\n",
    "        saver_load_obj = tf.train.Saver({'obj_ec_layer_weights':self.obj_ec_layer['weights'],\n",
    "                                         'obj_ec_layer_bias':self.obj_ec_layer['bias'],\n",
    "                                         'obj_dc_layer_weights':self.obj_dc_layer['weights'],\n",
    "                                         'obj_dc_layer_bias':self.obj_dc_layer['bias']})\n",
    "\n",
    "        saver_load_bld = tf.train.Saver({'bld_ec_layer_weights':self.bld_ec_layer['weights'],\n",
    "                                         'bld_ec_layer_bias':self.bld_ec_layer['bias'],\n",
    "                                         'bld_dc_layer_weights':self.bld_dc_layer['weights'],\n",
    "                                         'bld_dc_layer_bias':self.bld_dc_layer['bias']})\n",
    "\n",
    "        saver_load_veg = tf.train.Saver({'veg_ec_layer_weights':self.veg_ec_layer['weights'],\n",
    "                                         'veg_ec_layer_bias':self.veg_ec_layer['bias'],\n",
    "                                         'veg_dc_layer_weights':self.veg_dc_layer['weights'],\n",
    "                                         'veg_dc_layer_bias':self.veg_dc_layer['bias']})\n",
    "\n",
    "        saver_load_sky = tf.train.Saver({'sky_ec_layer_weights':self.sky_ec_layer['weights'],\n",
    "                                         'sky_ec_layer_bias':self.sky_ec_layer['bias'],\n",
    "                                         'sky_dc_layer_weights':self.sky_dc_layer['weights'],\n",
    "                                         'sky_dc_layer_bias':self.sky_dc_layer['bias']})\n",
    "\n",
    "\n",
    "\n",
    "        saver_save = tf.train.Saver()\n",
    "\n",
    "        dir = 'models/pretraining/20171013-212118'\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver_load_gnd.restore(sess,dir+'/pretrained_gnd.ckpt')\n",
    "            saver_load_obj.restore(sess,dir+'/pretrained_obj.ckpt')\n",
    "            saver_load_bld.restore(sess,dir+'/pretrained_bld.ckpt')\n",
    "            saver_load_veg.restore(sess,dir+'/pretrained_veg.ckpt')\n",
    "            saver_load_sky.restore(sess,dir+'/pretrained_sky.ckpt')\n",
    "\n",
    "            train_writer1 = tf.summary.FileWriter(self.FLAGS.logs_dir,sess.graph)\n",
    "\n",
    "            n_batches = int(len(self.gnd_train)/self.batch_size)\n",
    "\n",
    "            #tf.get_default_graph().finalize()\n",
    "\n",
    "            for epoch in range(0,self.hm_epochs):\n",
    "                sess.run(epoch_loss_reset)\n",
    "                time1 = datetime.datetime.now()\n",
    "\n",
    "                for _ in range(0,n_batches):\n",
    "\n",
    "                    imr_batch = self.imr_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    img_batch = self.img_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    imb_batch = self.imb_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    depth_batch = self.depth_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    gnd_batch = self.gnd_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    obj_batch = self.obj_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    bld_batch = self.bld_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    veg_batch = self.veg_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "                    sky_batch = self.sky_train[_*self.batch_size:(_+1)*self.batch_size]\n",
    "\n",
    "                    imr_in,img_in,imb_in,depth_in,gnd_in,obj_in,bld_in,veg_in,sky_in = input_distortion(copy(imr_batch),\n",
    "                                                                                                        copy(img_batch),\n",
    "                                                                                                        copy(imb_batch),\n",
    "                                                                                                        copy(depth_batch),\n",
    "                                                                                                        copy(gnd_batch),\n",
    "                                                                                                        copy(obj_batch),\n",
    "                                                                                                        copy(bld_batch),\n",
    "                                                                                                        copy(veg_batch),\n",
    "                                                                                                        copy(sky_batch),\n",
    "                                                                                                        resolution=(18,60))\n",
    "\n",
    "                    feed_dict_sem = {self.input_gnd:gnd_in,\n",
    "                                     self.label_gnd:gnd_batch,\n",
    "                                     self.input_obj:obj_in,\n",
    "                                     self.label_obj:obj_batch,\n",
    "                                     self.input_bld:bld_in,\n",
    "                                     self.label_bld:bld_batch,\n",
    "                                     self.input_veg:veg_in,\n",
    "                                     self.label_veg:veg_batch,\n",
    "                                     self.input_sky:sky_in,\n",
    "                                     self.label_sky:sky_batch}\n",
    "\n",
    "                    if epoch < 10:\n",
    "                        _, c = sess.run([opt1,epoch_loss_update],feed_dict=feed_dict_sem)\n",
    "                    else:\n",
    "                        _, c = sess.run([opt2,epoch_loss_update],feed_dict=feed_dict_sem)\n",
    "\n",
    "\n",
    "                sum_train = sess.run(sum_epoch_loss)\n",
    "                train_writer1.add_summary(sum_train,epoch)\n",
    "                print('Epoch', epoch, 'completed out of', self.hm_epochs)\n",
    "                print('Training Loss (per epoch): ', sess.run(epoch_loss.value()))\n",
    "\n",
    "                sess.run(loss_val_reset)\n",
    "\n",
    "                for i in set_val:\n",
    "                    label_imr = self.imr_val[i]\n",
    "                    label_img = self.img_val[i]\n",
    "                    label_imb = self.imb_val[i]\n",
    "                    label_depth = self.depth_val[i]\n",
    "                    label_gnd = self.gnd_val[i]\n",
    "                    label_obj = self.obj_val[i]\n",
    "                    label_bld = self.bld_val[i]\n",
    "                    label_veg = self.veg_val[i]\n",
    "                    label_sky = self.sky_val[i]\n",
    "\n",
    "                    imr_in,img_in,imb_in,depth_in,gnd_in,obj_in,bld_in,veg_in,sky_in = input_distortion(copy(label_imr),\n",
    "                                                                                                        copy(label_img),\n",
    "                                                                                                        copy(label_imb),\n",
    "                                                                                                        copy(label_depth),\n",
    "                                                                                                        copy(label_gnd),\n",
    "                                                                                                        copy(label_obj),\n",
    "                                                                                                        copy(label_bld),\n",
    "                                                                                                        copy(label_veg),\n",
    "                                                                                                        copy(label_sky),\n",
    "                                                                                                        resolution=(18,60),\n",
    "                                                                                                        singleframe=True)\n",
    "\n",
    "\n",
    "                    feed_dict_val = {self.input_gnd:gnd_in,\n",
    "                                     self.input_obj:obj_in,\n",
    "                                     self.input_bld:bld_in,\n",
    "                                     self.input_veg:veg_in,\n",
    "                                     self.input_sky:sky_in,\n",
    "                                     self.label_gnd:[label_gnd],\n",
    "                                     self.label_obj:[label_obj],\n",
    "                                     self.label_bld:[label_bld],\n",
    "                                     self.label_veg:[label_veg],\n",
    "                                     self.label_sky:[label_sky]}\n",
    "\n",
    "                    l,l_up = sess.run([loss,loss_val_update],feed_dict=feed_dict_val)\n",
    "\n",
    "                sum_val = sess.run(sum_val_loss)\n",
    "                train_writer1.add_summary(sum_val,epoch)\n",
    "                print('Validation Loss (per pixel): ', sess.run(val_loss.value())/set_val.shape[0])\n",
    "                time2 = datetime.datetime.now()\n",
    "                delta = time2-time1\n",
    "                print('Epoch Time [seconds]:', delta.seconds)\n",
    "                print('-----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if self.saving==True:\n",
    "                saver_save.save(sess,self.FLAGS.train_dir+'/pretrained_shared_semantics.ckpt')\n",
    "                print('SAVED MODEL')\n",
    "\n",
    "    def validate_depth(self,run=False):\n",
    "\n",
    "        prediction = self.AE_depth(self.input_depth)\n",
    "        loss = tf.nn.l2_loss(tf.multiply(self.depth_mask,prediction)-tf.multiply(self.depth_mask,self.label_depth))\n",
    "\n",
    "        load_weights = tf.train.Saver()\n",
    "\n",
    "        if run==False:\n",
    "            raise ValueError\n",
    "\n",
    "        dir = 'models/pretraining/' + run\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            load_weights.restore(sess,dir+'/pretrained_depth.ckpt')\n",
    "\n",
    "\n",
    "\n",
    "            for i in range(0,self.n_validation_data):\n",
    "\n",
    "\n",
    "                depth_label = self.depth_val[i]\n",
    "                depth_mask = self.depth_mask_val[i]\n",
    "                depth_input = pretraining_input_distortion(copy(depth_label),singleframe=True)\n",
    "\n",
    "                normalization = np.count_nonzero(depth_mask)\n",
    "\n",
    "                feed_dict = {self.input_depth:depth_input,\n",
    "                             self.label_depth:[depth_label],\n",
    "                             self.depth_mask:[depth_mask]}\n",
    "\n",
    "                depth_pred, l = sess.run([prediction,loss],feed_dict=feed_dict)\n",
    "                print_validation_frames(depth_input,depth_pred,depth_label,channel='depth',shape=(60,18))\n",
    "\n",
    "\n",
    "                print('Validation Loss:', l/normalization)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pretraining = PretrainingMAE(data_train, data_validate, data_test)\n",
    "\n",
    "#pretraining.pretrain_red_channel()\n",
    "#pretraining.pretrain_green_channel()\n",
    "#pretraining.pretrain_blue_channel()\n",
    "\n",
    "#pretraining.pretrain_gnd_channel()\n",
    "#pretraining.pretrain_obj_channel()\n",
    "#pretraining.pretrain_bld_channel()\n",
    "#pretraining.pretrain_veg_channel()\n",
    "#pretraining.pretrain_sky_channel()\n",
    "pretraining.pretrain_shared_semantics()\n",
    "pretraining.pretrain_depth_channel()\n",
    "\n",
    "#pretraining.validate_depth(run='20171010-115125')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from load_data import load_data\n",
    "from input_distortion import input_distortion,pretraining_input_distortion\n",
    "from visualization import print_training_frames,print_validation_frames\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "data_train,data_validate,data_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PretrainingMAE():\n",
    "\n",
    "    def __init__(self,data_train,data_validate,data_test):\n",
    "\n",
    "        # storing data\n",
    "        self.data_train = data_train\n",
    "        self.data_val = data_validate\n",
    "        self.data_test = data_test\n",
    "\n",
    "        # training options\n",
    "\n",
    "        self.batch_size = 100\n",
    "        self.hm_epochs = 150\n",
    "\n",
    "        self.input_size = 1080\n",
    "        self.hidden_size = 1024\n",
    "\n",
    "        self.saving = True\n",
    "        self.n_training_data = 'all'\n",
    "\n",
    "        self.decay = 'constant'\n",
    "\n",
    "        self.prepare_training_data()\n",
    "        self.prepare_validation_data()\n",
    "\n",
    "        self.n_validation_data = len(self.imr_val)\n",
    "        self.n_training_validations = 100\n",
    "\n",
    "        self.input_red =tf.placeholder('float', shape=[None, self.input_size])\n",
    "        self.input_green = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_blue = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_depth = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_gnd = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_obj = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_bld = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_veg = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.input_sky = tf.placeholder('float',shape=[None,self.input_size])\n",
    "\n",
    "        self.depth_mask = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.depth_loss_mask = tf.placeholder('float',shape=[None,self.input_size])\n",
    "\n",
    "        self.label_red = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_green = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_blue = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_depth = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_gnd = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_obj = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_bld = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_veg = tf.placeholder('float',shape=[None,self.input_size])\n",
    "        self.label_sky = tf.placeholder('float',shape=[None,self.input_size])\n",
    "\n",
    "        # loss options\n",
    "\n",
    "\n",
    "        # layers container\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        self.mode = 'pretraining/'\n",
    "\n",
    "        self.model_folder = 'models/'\n",
    "        self.logs_folder = 'logs/'\n",
    "        self.run = now.strftime('%Y%m%d-%H%M%S')\n",
    "        #self.run = '20171013-212118'\n",
    "\n",
    "        # directory definitions\n",
    "\n",
    "\n",
    "        self.project_dir ='./'\n",
    "        self.model_dir = self.project_dir + self.model_folder + self.mode + self.run\n",
    "        self.logs_dir = self.project_dir + self.logs_folder + self.mode + self.run\n",
    "\n",
    "        os.mkdir(self.model_dir)\n",
    "        os.mkdir(self.logs_dir)\n",
    "\n",
    "        tf.app.flags.DEFINE_string('train_dir',self.model_dir,'where to store the trained model')\n",
    "        tf.app.flags.DEFINE_string('logs_dir',self.logs_dir,'where to store the summaries')\n",
    "\n",
    "\n",
    "        self.FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        '''\n",
    "        Function for bringing the training data into a form that suits the training process\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        self.imr_train = []\n",
    "        self.img_train = []\n",
    "        self.imb_train = []\n",
    "        self.depth_train = []\n",
    "        self.gnd_train = []\n",
    "        self.obj_train = []\n",
    "        self.bld_train = []\n",
    "        self.veg_train = []\n",
    "        self.sky_train = []\n",
    "\n",
    "        self.depth_mask_train = []\n",
    "        self.depth_loss_mask_train = []\n",
    "\n",
    "        t_iterator = 0\n",
    "\n",
    "        for i in self.data_train:\n",
    "            for j in i:\n",
    "                if t_iterator == self.n_training_data:\n",
    "                    break\n",
    "                self.imr_train.append(j['xcr1']/255.)\n",
    "                self.img_train.append(j['xcg1']/255.)\n",
    "                self.imb_train.append(j['xcb1']/255.)\n",
    "                self.depth_train.append(j['xid1'])\n",
    "                self.depth_mask_train.append(j['xmask1'])\n",
    "                self.depth_loss_mask_train.append((j['xid1']>0.09).astype(int))\n",
    "                self.gnd_train.append((j['sem1']==1).astype(int))\n",
    "                self.obj_train.append((j['sem1']==2).astype(int))\n",
    "                self.bld_train.append((j['sem1']==3).astype(int))\n",
    "                self.veg_train.append((j['sem1']==4).astype(int))\n",
    "                self.sky_train.append((j['sem1']==5).astype(int))\n",
    "\n",
    "\n",
    "\n",
    "                t_iterator += 1\n",
    "\n",
    "        t_iterator = 0\n",
    "        for i in self.data_train:\n",
    "            for j in i:\n",
    "                if t_iterator == self.n_training_data:\n",
    "                    break\n",
    "                self.imr_train.append(j['xcr2']/255.)\n",
    "                self.img_train.append(j['xcg2']/255.)\n",
    "                self.imb_train.append(j['xcb2']/255.)\n",
    "                self.depth_train.append(j['xid2'])\n",
    "                self.depth_mask_train.append(j['xmask2'])\n",
    "                self.depth_loss_mask_train.append((j['xid2']>0.09).astype(int))\n",
    "                self.gnd_train.append((j['sem2']==1).astype(int))\n",
    "                self.obj_train.append((j['sem2']==2).astype(int))\n",
    "                self.bld_train.append((j['sem2']==3).astype(int))\n",
    "                self.veg_train.append((j['sem2']==4).astype(int))\n",
    "                self.sky_train.append((j['sem2']==5).astype(int))\n",
    "\n",
    "                t_iterator += 1\n",
    "\n",
    "        # randomly shuffle input frames\n",
    "        rand_indices = np.arange(len(self.imr_train)).astype(int)\n",
    "        np.random.shuffle(rand_indices)\n",
    "\n",
    "        self.imr_train = np.asarray(self.imr_train)[rand_indices]\n",
    "        self.img_train = np.asarray(self.img_train)[rand_indices]\n",
    "        self.imb_train = np.asarray(self.imb_train)[rand_indices]\n",
    "        self.depth_train = np.asarray(self.depth_train)[rand_indices]\n",
    "        self.gnd_train = np.asarray(self.gnd_train)[rand_indices]\n",
    "        self.obj_train = np.asarray(self.obj_train)[rand_indices]\n",
    "        self.bld_train = np.asarray(self.bld_train)[rand_indices]\n",
    "        self.veg_train = np.asarray(self.veg_train)[rand_indices]\n",
    "        self.sky_train = np.asarray(self.sky_train)[rand_indices]\n",
    "        self.depth_mask_train = np.asarray(self.depth_mask_train)[rand_indices]\n",
    "        self.depth_loss_mask_train = np.asarray(self.depth_loss_mask_train)[rand_indices]\n",
    "\n",
    "    def prepare_validation_data(self):\n",
    "\n",
    "            # prepare validation data containers\n",
    "            self.imr_val = []\n",
    "            self.img_val = []\n",
    "            self.imb_val = []\n",
    "            self.depth_val = []\n",
    "            self.gnd_val = []\n",
    "            self.obj_val = []\n",
    "            self.bld_val = []\n",
    "            self.veg_val = []\n",
    "            self.sky_val = []\n",
    "            self.depth_mask_val = []\n",
    "            self.depth_loss_mask_val = []\n",
    "\n",
    "            v_iterator = 0\n",
    "\n",
    "            for i in self.data_val:\n",
    "                for j in i:\n",
    "                    self.imr_val.append(j['xcr1']/255.)\n",
    "                    self.img_val.append(j['xcg1']/255.)\n",
    "                    self.imb_val.append(j['xcb1']/255.)\n",
    "                    self.depth_val.append(j['xid1'])\n",
    "                    self.depth_loss_mask_val.append((j['xid1']>0.1).astype(int))\n",
    "                    self.gnd_val.append((j['sem1']==1).astype(int))\n",
    "                    self.obj_val.append((j['sem1']==2).astype(int))\n",
    "                    self.bld_val.append((j['sem1']==3).astype(int))\n",
    "                    self.veg_val.append((j['sem1']==4).astype(int))\n",
    "                    self.sky_val.append((j['sem1']==5).astype(int))\n",
    "                    self.depth_mask_val.append(j['xmask1'])\n",
    "\n",
    "                    v_iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/pretraining/20171016-214543'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3dfaa3850457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainingMAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-cfced0648731>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_train, data_validate, data_test)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/pretraining/20171016-214543'"
     ]
    }
   ],
   "source": [
    "pretraining = PretrainingMAE(data_train, data_validate, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
