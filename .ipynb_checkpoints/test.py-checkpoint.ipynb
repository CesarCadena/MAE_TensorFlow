{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 349)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m349\u001b[0m\n\u001b[0;31m    z_out=sess.run(vae_Full.x_reconstr_mean,feed_dict={vae_Full.x:z_in[0:100,:]})\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#%load VAE_full.py\n",
    "#This is VAE_depth script \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.4\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "\t\"\"\" Xavier initialization of network weights\"\"\"\n",
    "\t# https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "\tlow = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "\thigh = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "\t# return tensors\n",
    "\treturn tf.random_uniform((fan_in, fan_out), #  shape of the weights\n",
    "\t\t\t\t\t\t\t minval=low, maxval=high, # here is the range \n",
    "\t\t\t\t\t\t\t dtype=tf.float32) # here is the type\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "\t\"\"\" Based on See \"Auto-Encoding Variational Bayes\" by Kingma and Welling\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, network_architecture,\n",
    "\t\t\t\t transfer_fct=tf.nn.softplus,learning_rate=1e-3,batch_size=100):\n",
    "\t\tself.network_architecture=network_architecture# which is a dictionary \n",
    "\t\tself.transfer_fct=transfer_fct\n",
    "\t\tself.learning_rate=learning_rate\n",
    "\t\tself.batch_size=batch_size\n",
    "\t\t# tf Graph input\n",
    "\t\tself.x=tf.placeholder(tf.float32,[None, network_architecture[\"n_input\"]])\n",
    "\t\t# Create auotencoder \n",
    "\t\tself.create_network()\n",
    "\t\t# define loss function based on variational upper bound \n",
    "\t\t# and corresponding optimizer \n",
    "\t\tself.create_loss_optimizer()\n",
    "\t\t#self.saver=tf.train.Saver()\n",
    "\n",
    "\n",
    "\tdef initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2,\n",
    "\t\t\t\t\t\t\tn_hidden_gener_1,n_hidden_gener_2,\n",
    "\t\t\t\t\t\t\tn_input, n_z):\n",
    "\t\t# create a dictionary of tensor variables \n",
    "\t\tall_weights = dict()\n",
    "\t\t# recognition  network\n",
    "\t\tall_weights['weights_recog'] = {\n",
    "\t\t\t'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "\t\t\t'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "\t\t\t'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "\t\t\t'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "\t\tall_weights['biases_recog'] = {\n",
    "\t\t\t'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "\t\t\t'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "\t\t\t'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "\t\t\t'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "\t\t# generate network \n",
    "\t\tall_weights['weights_gener'] = {\n",
    "\t\t\t'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "\t\t\t'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "\t\t\t'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "\t\t\t'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "\t\tall_weights['biases_gener'] = {\n",
    "\t\t\t'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "\t\t\t'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "\t\t\t'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "\t\t\t'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "\t\treturn all_weights\n",
    "\t\n",
    "\tdef create_network(self):\n",
    "\t\t# create tensor variables for  weights and bias\n",
    "\t\tself.network_weights=self.initialize_weights(**self.network_architecture) \n",
    "\t\t# network_weights  is a dictionary \n",
    "\t\t# pass architecture parameters \n",
    "\t\t# network_architecture is a dictionary \n",
    "\n",
    "\t\t# recognition network :\n",
    "\t\t#input: data x shape [batch_size,n_x]\n",
    "\t\t#output : mean of z , log(variance^2) shape [batch_size,n_z]\n",
    "\t\t# pass variables to network \n",
    "\t\tself.z_mean, self.z_log_sigma_sq = \\\n",
    "\t\t\tself.recognition_network(self.network_weights[\"weights_recog\"], \n",
    "\t\t\t\t\t\t\t\t\t self.network_weights[\"biases_recog\"])\n",
    "\t\t\t\n",
    "\t\tn_z = self.network_architecture[\"n_z\"]# dimension of z\n",
    "\t\t\n",
    "\t\t\n",
    "\t\teps = tf.random_normal((self.batch_size, n_z), 0, 1, dtype=tf.float32) \n",
    "\t\t# standard Normal\n",
    "\t\t# z = z_mean + z_sigma*epsilon\n",
    "\t\t\n",
    "\t\tself.z = tf.add(self.z_mean, tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\t\t#shape [batch_size,n_z]\n",
    "\t\t\n",
    "\t\t# Generate network :\n",
    "\t\t# input z\n",
    "\t\t# output mean of pixels shape[batch_Size,n_x]\n",
    "\t\t# multivariant Gaussian Distribution\n",
    "\t\tself.x_reconstr_mean = \\\n",
    "\t\t\tself.generator_network(self.network_weights[\"weights_gener\"],\n",
    "\t\t\t\t\t\t\t\t   self.network_weights[\"biases_gener\"])\n",
    "\t\n",
    "\tdef recognition_network(self, weights, biases):\n",
    "\t\t# Generate probabilistic encoder (recognition network), which\n",
    "\t\t# maps inputs onto a normal distribution in latent space.\n",
    "\t\t# The transformation is parametrized and can be learned.\n",
    "\t\tlayer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "\t\t\t\t\t\t\t\t\t\t   biases['b1'])) \n",
    "\t\tlayer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "\t\t\t\t\t\t\t\t\t\t   biases['b2'])) \n",
    "\t\tz_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "\t\t\t\t\t\tbiases['out_mean'])\n",
    "\t\t\n",
    "\t\tz_log_sigma_sq =\\\n",
    "\t\t\ttf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "\t\t\t\t   biases['out_log_sigma'])\n",
    "\t\t\t\n",
    "\t\treturn (z_mean, z_log_sigma_sq)\n",
    "\t\n",
    "\t# use variables to buld generate network \n",
    "\tdef generator_network(self, weights, biases):\n",
    "\t\t# Generate probabilistic decoder (decoder network), which\n",
    "\t\t# maps points in latent space onto a Bernoulli distribution in data space.\n",
    "\t\t# The transformation is parametrized and can be learned.\n",
    "\t\tlayer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "\t\t\t\t\t\t\t\t\t\t   biases['b1'])) \n",
    "\t\tlayer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "\t\t\t\t\t\t\t\t\t\t   biases['b2'])) \n",
    "\t\t\n",
    "\t\t# depth estimation mean \n",
    "\t\tx_reconstr_mean = \\\n",
    "\t\t   tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
    "\t\t\t\t\t\t\t\t biases['out_mean'])\n",
    "\t\t#x_reconstr_sigma= \\\n",
    "\t\t#     tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "\t\t#                        biases['out_log_sigma'])\n",
    "\t\treturn x_reconstr_mean\n",
    "\t\n",
    "\tdef create_loss_optimizer(self):\n",
    "\t\t# The loss is composed of two terms:\n",
    "\t\t\n",
    "\t\t# 1.) The reconstruction loss (the negative log probability\n",
    "\t\t#     of the input under the reconstructed Bernoulli/Gaussian distribution \n",
    "\t\t#     induced by the decoder in the data space).\n",
    "\t\t#     This can be interpreted as the number of \"nats\" required\n",
    "\t\t#     for reconstructing the input when the activation in latent\n",
    "\t\t#     is given.\n",
    "\t\t# Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "\t\t# Assuem identity gaussian \n",
    "\t\t\n",
    "\t\t# loss from generative data \n",
    " \n",
    "\t\t# 1) bernouli distribution\n",
    "\t\t\"\"\"\n",
    "\t\treconstr_loss =-tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "\t\t\t\t\t\t   + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "\t\t\t\t\t\t   axis=1)\n",
    "\t\t\"\"\"\n",
    "\t\t# 1) gaussian distribution\n",
    "\t\treconstr_error=self.x-self.x_reconstr_mean\n",
    "\t\treconstr_loss=tf.reduce_sum(tf.square(reconstr_error),axis=1)\n",
    "\t\t# 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "\t\t##    between the distribution in latent space induced by the encoder on \n",
    "\t\t#     the data and some prior. This acts as a kind of regularizer.\n",
    "\t\t#     This can be interpreted as the number of \"nats\" required\n",
    "\t\t#     for transmitting the the latent space distribution given\n",
    "\t\t#     the prior.\n",
    "\t\t#     closed form of  KL  divergence with gaussian distribution\n",
    "\t\tlatent_loss=-0.5*tf.reduce_sum(1+self.z_log_sigma_sq \n",
    "\t\t\t\t\t\t\t\t\t\t   -tf.square(self.z_mean) \n",
    "\t\t\t\t\t\t\t\t\t\t   -tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "\t\tself.cost = tf.reduce_mean(reconstr_loss + latent_loss) # average over batch\n",
    "\t\t# Use ADAM optimizer\n",
    "\t\tself.optimizer = \\\n",
    "\t\t\ttf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\t\t\t\n",
    "\n",
    "\tdef partial_fit(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tTrain model based on mini-batch of input data.\n",
    "\t\tReturn cost of mini-batch.\n",
    "\t\t\"\"\"\n",
    "\t\topt,cost = sess.run((self.optimizer, self.cost), \n",
    "\t\t\t\t\t\t\t\t  feed_dict={self.x: X})\n",
    "\t\treturn cost\n",
    "\n",
    "\n",
    "\tdef train(self, batch_size=100, training_epochs=10, display_step=1):\n",
    "\t\tprint(\"training started ...\")\n",
    "\t\ttrain_indices=range(n_samples)\n",
    "\t\tfor epoch in range(training_epochs):\n",
    "\t\t\tavg_cost = 0.\n",
    "\t\t\ttotal_batch = int(n_samples / batch_size)\n",
    "\t\t\tperm_indices=np.random.permutation(train_indices)\n",
    "\t\t# Loop over all batches\n",
    "\t\t\tfor i in range(total_batch):\n",
    "\t\t\t\toffset=(i*batch_size)%(n_samples-batch_size)\n",
    "\t\t\t\t# mnist data  batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\t\t\t\tbatch_indices=perm_indices[offset:(offset+batch_size)]\n",
    "\t\t\t\t# feed the data for full models \n",
    "\t\t\t\tbatch_xs=z_in[batch_indices]\n",
    "\t\t\t# Fit training using batch data\n",
    "\t\t\t\tcost = self.partial_fit(batch_xs)\n",
    "\t\t\t# Compute average loss\n",
    "\t\t\t\tavg_cost += cost/n_samples*batch_size\n",
    "\t\t# Display logs per epoch step\n",
    "\t\t\tif epoch % display_step == 0:\n",
    "\t\t\t\tprint(\"Epoch:\", '%04d' % (epoch+1), \n",
    "\t\t\t\t\t  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Build  network for depth channel\n",
    "with tf.variable_scope(\"depth\"):\n",
    "\tnetwork_architecture_depth= \\\n",
    "\t   dict(n_hidden_recog_1=1000, # 1st layer encoder neurons\n",
    "\t\t n_hidden_recog_2=1000, # 2nd layer encoder neurons\n",
    "\t\t n_hidden_gener_1=1000, # 1st layer decoder neurons\n",
    "\t\t n_hidden_gener_2=1000, # 2nd layer decoder neurons\n",
    "\t\t n_input=1080, # MNIST data input (img shape: 28*28)\n",
    "\t\t n_z=50)  # dimensionality of latent space\n",
    "\tvae_depth=VariationalAutoencoder(network_architecture_depth,learning_rate=0.001,batch_size=100)\n",
    "\n",
    "listvar=vae_depth.network_weights\n",
    "var_depth=(list(listvar['weights_recog'].values())\n",
    "\t+list(listvar['biases_recog'].values())\n",
    "\t+list(listvar['weights_gener'].values())\n",
    "\t+list(listvar['biases_gener'].values()))\n",
    "saver_depth=tf.train.Saver(var_depth)\n",
    "\n",
    "\n",
    "# Build network for semantics  channel\n",
    "with tf.variable_scope(\"RGB\"):\n",
    "\tnetwork_architecture_rgb= \\\n",
    "\t\tdict(n_hidden_recog_1=1000, # 1st layer encoder neurons\n",
    "\t\t\tn_hidden_recog_2=1000, # 2nd layer encoder neurons\n",
    "\t\t\tn_hidden_gener_1=1000, # 1st layer decoder neurons\n",
    "\t\t\tn_hidden_gener_2=1000, # 2nd layer decoder neurons\n",
    "\t\t\tn_input=1080*3, # MNIST data input (img shape: 28*28)\n",
    "\t\t\tn_z=50)  # dimensionality of latent space\n",
    "\tvae_rgb=VariationalAutoencoder(network_architecture_rgb,learning_rate=0.001,batch_size=100)\n",
    "  \n",
    "listvar2=vae_rgb.network_weights\n",
    "var_rgb=(list(listvar2['weights_recog'].values())\n",
    "\t+list(listvar2['biases_recog'].values())\n",
    "\t+list(listvar2['weights_gener'].values())\n",
    "\t+list(listvar2['biases_gener'].values()))\n",
    "saver_rgb=tf.train.Saver(var_rgb)\n",
    "\n",
    "\n",
    "# Build network for semantic channels\n",
    "with tf.variable_scope(\"Sem\"):\n",
    "\tnetwork_architecture_Sem = \\\n",
    "\t\tdict(n_hidden_recog_1=2000, # 1st layer encoder neurons\n",
    "\t\t n_hidden_recog_2=2000, # 2nd layer encoder neurons\n",
    "\t\t n_hidden_gener_1=2000, # 1st layer decoder neurons\n",
    "\t\t n_hidden_gener_2=2000, # 2nd layer decoder neurons\n",
    "\t\t n_input=5400, # MNIST data input (img shape: 28*28)\n",
    "\t\t n_z=100)  # dimensionality of latent space\n",
    "\tvae_Sem=VariationalAutoencoder(network_architecture_Sem,learning_rate=1e-4,batch_size=100)\n",
    "\n",
    "listvar3=vae_Sem.network_weights\n",
    "var_Sem=(list(listvar3['weights_recog'].values())\n",
    "\t+list(listvar3['biases_recog'].values())\n",
    "\t+list(listvar3['weights_gener'].values())\n",
    "\t+list(listvar3['biases_gener'].values()))\n",
    "saver_Sem=tf.train.Saver(var_Sem)\n",
    "\n",
    "\n",
    "########################    Start build  shared information fusion   #############################\n",
    "\n",
    "\n",
    "\n",
    "########################    Start build  shared information fusion   #############################\n",
    "############## Load data ####################\n",
    "depth_data=np.load(\"../depth_data.npy\")\n",
    "Depth_input=np.transpose(depth_data,(0,2,1,3))[:,:,:,0].reshape(-1,1080)# shape [size,1080]\n",
    "\n",
    "RGB_data=np.load(\"../rgb_data.npy\")\n",
    "R_data=RGB_data[:,:,:,0].reshape(-1,1080)\n",
    "G_data=RGB_data[:,:,:,1].reshape(-1,1080)\n",
    "B_data=RGB_data[:,:,:,2].reshape(-1,1080)\n",
    "RGB_input=np.concatenate((R_data,G_data,B_data),axis=1) #shape[size,3*1080]\n",
    "\n",
    "Sem_data=np.load(\"../sem_data.npy\")\n",
    "Sem_input=np.transpose(Sem_data,(0,2,1,3))\n",
    "Ground_input=Sem_input[:,:,:,0].reshape(-1,1080)\n",
    "Objects_input=Sem_input[:,:,:,1].reshape(-1,1080)\n",
    "Building_input=Sem_input[:,:,:,2].reshape(-1,1080)\n",
    "Vegetation_input=Sem_input[:,:,:,3].reshape(-1,1080)\n",
    "Sky_input=Sem_input[:,:,:,4].reshape(-1,1080)\n",
    "Sem_input=np.concatenate((Ground_input,Objects_input,\n",
    "\t\t\t\t\t\t  Building_input,Vegetation_input,Sky_input),\n",
    "\t\t\t\t\t\t  axis=1)# shape[size,5*1080]\n",
    "\n",
    "n_samples=Sem_input.shape[0] # size \n",
    "############## Finish Load data ####################\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"Full\"):\n",
    "\tnetwork_architecture_Full = \\\n",
    "\t\tdict(n_hidden_recog_1=50, # 1st layer encoder neurons\n",
    "\t\t n_hidden_recog_2=50, # 2nd layer encoder neurons\n",
    "\t\t n_hidden_gener_1=50, # 1st layer decoder neurons\n",
    "\t\t n_hidden_gener_2=50, # 2nd layer decoder neurons\n",
    "\t\t n_input=200, # MNIST data input (img shape: 28*28)\n",
    "\t\t n_z=100)  # dimensionality of latent space\n",
    "\t#vae_Sem= VariationalAutoencoder(network_architecture_Sem,learning_rate=1e-4, batch_size=100)\n",
    "\tvae_Full=VariationalAutoencoder(network_architecture_Full,learning_rate=1e-4,batch_size=100)\n",
    "\n",
    "\n",
    "\n",
    "### Initialization\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "###Load  other models \n",
    "saver_depth.restore(sess,\"models/depth_100_epochs/model\")\n",
    "print(\"loaded model weights from \"+\"models/depth_100_epochs/model\")\n",
    "\n",
    "saver_rgb.restore(sess,\"models/RGB_100_epochs/model\")\n",
    "print(\"loaded model weights from \"+\"models/RGB_100_epochs/model\")\n",
    "\n",
    "saver_Sem.restore(sess,\"models/sem_100_epochs/model\")\n",
    "print(\"loaded model weights from \"+\"models/sem_100_epochs/model\")\n",
    "\n",
    "\n",
    "###Build data for full model \n",
    "z_depth=sess.run(vae_depth.z_mean,feed_dict={vae_depth.x:Depth_input})\n",
    "z_rgb=sess.run(vae_rgb.z_mean,feed_dict={vae_rgb.x:RGB_input})\n",
    "z_sem=sess.run(vae_Sem.z_mean,feed_dict={vae_Sem.x:Sem_input})\n",
    "\n",
    "#z_in is the data required \n",
    "z_in=np.concatenate((z_rgb,z_depth,z_sem),axis=1) #  shape[30602,200] \n",
    "\n",
    "\n",
    "######################## variables list #########################\n",
    "listvar4=vae_Full.network_weights\n",
    "var_Full=(list(listvar4['weights_recog'].values())\n",
    "\t+list(listvar4['biases_recog'].values())\n",
    "\t+list(listvar4['weights_gener'].values())\n",
    "\t+list(listvar4['biases_gener'].values()))\n",
    "saver_Full=tf.train.Saver(var_Full)\n",
    "\n",
    "train_new_model=False\n",
    "if train_new_model:    \n",
    "\tvae_Full.train(batch_size=100, training_epochs=1)\n",
    "\tsaver_Full.save(sess,\"models/full_100_epochs/model\")\n",
    "\tprint(\"saved the vae model weights to \"+\"models/full_100_epochs/model\")\n",
    "else:\n",
    "    \n",
    "\tsaver_Full.restore(sess,\"models/full_100_epochs/model\")\n",
    "\tprint(\"loaded the vae model weights from\"+\"models/full_100_epochs/model\")\n",
    "    # test and visualization\n",
    "    \n",
    "    z_out=sess.run(vae_Full.x_reconstr_mean,feed_dict={vae_Full.x:z_in[0:100,:]})\n",
    "    z_out_rgb,z_out_depth,z_out_sem=np.split(z_out, [50,100],axis=1)\n",
    "\n",
    "    rgb_out=sess.run(vae_rgb.x_reconstr_mean,feed_dict={vae_rgb.z:z_out_rgb})# shape [size,3240]\n",
    "    depth_image=sess.run(vae_depth.x_reconstr_mean,feed_dict={vae_depth.z:z_out_depth})# shape [size,1080]\n",
    "    sem_out=sess.run(vae_Sem.x_reconstr_mean,feed_dict={vae_Sem.z:z_out_sem})# shape[size,5400]\n",
    "\n",
    "    red_out,green_out,blue_out=np.split(rgb_out,3,axis=1)\n",
    "    RGB_image=np.dstack((red_out,green_out,blue_out))\n",
    "\n",
    "    G_out,O_out,V_out,B_out,S_out=np.split(sem_out,5,axis=1)\n",
    "    Sem_image=np.dstack((G_out,O_out,V_out,B_out,S_out))\n",
    "    plt.imshow(np.reshape(depth_image[0],(18,60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x198f03f828>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACHCAYAAAAGEqJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbNCNMaYSbNCNMaYSbNCNMaYS/g8T3igSOh/xYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b4386b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(depth_image[0],(18,60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19b7c024e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACHCAYAAAAGEqJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEyQQRdCiEyQQRdCiEyQQRdCiEz4f3/PwjHzDd+iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b7dc3ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rgb_out[0,2160:3240].reshape(18,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
