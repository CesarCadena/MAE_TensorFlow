{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started ...\n",
      "Epoch: 0001 cost= 191.617001889\n",
      "Epoch: 0002 cost= 128.386782347\n",
      "Epoch: 0003 cost= 109.789870529\n",
      "Epoch: 0004 cost= 102.487287352\n",
      "Epoch: 0005 cost= 98.814927060\n",
      "Epoch: 0006 cost= 95.521391237\n",
      "Epoch: 0007 cost= 92.817185129\n",
      "Epoch: 0008 cost= 90.225630715\n",
      "Epoch: 0009 cost= 88.451511150\n",
      "Epoch: 0010 cost= 86.747137246\n",
      "saved the vae model weights to vae_models/RGB_10_epochs/model\n"
     ]
    }
   ],
   "source": [
    "# %load VAE_RGB.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.4\n",
    "\n",
    "\n",
    "# Initialization\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    # return tensors\n",
    "    return tf.random_uniform((fan_in, fan_out), #  shape of the weights\n",
    "                             minval=low, maxval=high, # here is the range \n",
    "                             dtype=tf.float32) # here is the type \n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    \n",
    "    \"\"\" Based on See \"Auto-Encoding Variational Bayes\" by Kingma and Welling\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture,\n",
    "                 transfer_fct=tf.nn.softplus,learning_rate=1e-3,batch_size=100):\n",
    "        \n",
    "        \n",
    "        self.network_architecture=network_architecture# which is a dictionary \n",
    "        self.transfer_fct=transfer_fct\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size=batch_size\n",
    "        # tf Graph input\n",
    "        self.x=tf.placeholder(tf.float32,[None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        \n",
    "        # Create auotencoder \n",
    "        self.create_network()\n",
    "        # define loss function based on variational upper bound \n",
    "        # and corresponding optimizer \n",
    "        self.create_loss_optimizer()\n",
    "        # initial the tensorflow variables \n",
    "        init=tf.global_variables_initializer()\n",
    "        self.saver_r=tf.train.Saver()\n",
    "        self.sess=tf.Session(config=config)\n",
    "        self.sess.run(init)\n",
    "    \n",
    "            \n",
    "    def initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                            n_input, n_z):\n",
    "        \n",
    "        # create a dictionary of tensor variables \n",
    "        all_weights = dict()\n",
    "        \n",
    "        # recognition  network\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        \n",
    "        # generate network \n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "        \n",
    "        \n",
    "    def create_network(self):\n",
    "        # create tensor variables for  weights and bias\n",
    "        network_weights=self.initialize_weights(**self.network_architecture) \n",
    "        # pass architecture parameters \n",
    "        # network_architecture is a dictionary \n",
    "        \n",
    "        \n",
    "        # recognition network :\n",
    "        #input: data x shape [batch_size,n_x]\n",
    "        #output : mean of z , log(variance^2) shape [batch_size,n_z]\n",
    "        # pass variables to network \n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self.recognition_network(network_weights[\"weights_recog\"], \n",
    "                                     network_weights[\"biases_recog\"])\n",
    "            \n",
    "        n_z = self.network_architecture[\"n_z\"]# dimension of z\n",
    "        \n",
    "        \n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, dtype=tf.float32) \n",
    "        # standard Normal\n",
    "        # z = z_mean + z_sigma*epsilon\n",
    "        \n",
    "        self.z = tf.add(self.z_mean, tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "        #shape [batch_size,n_z]\n",
    "        \n",
    "        # Generate network :\n",
    "        # input z\n",
    "        # output mean of pixels shape[batch_Size,n_x]\n",
    "        # multivariant Gaussian Distribution\n",
    "        self.x_reconstr_mean = \\\n",
    "            self.generator_network(network_weights[\"weights_gener\"],\n",
    "                                   network_weights[\"biases_gener\"])\n",
    "            \n",
    "\n",
    "    #  use the variables to build the network      \n",
    "    def recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        \n",
    "        z_log_sigma_sq =\\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "            \n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "    \n",
    "    # use variables to buld generate network \n",
    "    def generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        \n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
    "                                 biases['out_mean']))\n",
    "            \n",
    "        #x_reconstr_sigma= \\\n",
    "        #     tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "        #                        biases['out_log_sigma'])\n",
    "            \n",
    "        return x_reconstr_mean\n",
    "    \n",
    "    \n",
    "    def create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        \n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli/Gaussian distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        # Assuem identity gaussian \n",
    "        \n",
    "        # loss from generative data \n",
    "        \n",
    "        # 1) bernouli distribution\n",
    "        \"\"\"\n",
    "        reconstr_loss =-tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "                           axis=1)\n",
    "        \"\"\"\n",
    "        # 1) gaussian distribution\n",
    "        reconstr_error=self.x-self.x_reconstr_mean\n",
    "        reconstr_loss=tf.reduce_sum(tf.square(reconstr_error),axis=1)\n",
    "         \n",
    "            \n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        #     closed form of  KL  divergence with gaussian distribution\n",
    "        latent_loss=-0.5*tf.reduce_sum(1+self.z_log_sigma_sq \n",
    "                                           -tf.square(self.z_mean) \n",
    "                                           -tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        \n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss) # average over batch\n",
    "        # Use ADAM optimizer\n",
    "        \n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})\n",
    "    \n",
    "    \n",
    "    def save(self, check_point_file = 'model.ckpt'):\n",
    "        save_path = self.saver_r.save(self.sess, check_point_file) # Saves the weights (not the graph)\n",
    "        print(\"saved the vae model weights to \"+save_path)\n",
    "        # to load it,\n",
    "        \n",
    "        \n",
    "    def load(self, check_point_file = 'model.ckpt'):\n",
    "        self.saver_r.restore(self.sess, check_point_file)\n",
    "        print(\"loaded model weights from \"+check_point_file)\n",
    "        \n",
    "    \n",
    "    def train(self, batch_size=100, training_epochs=10, display_step=1):\n",
    "        print(\"training started ...\")\n",
    "        train_indices=range(n_samples)\n",
    "        \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(n_samples / batch_size)\n",
    "            perm_indices=np.random.permutation(train_indices)\n",
    "        # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                offset=(i*batch_size)%(n_samples-batch_size)\n",
    "                # mnist data  batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "                batch_indices=perm_indices[offset:(offset+batch_size)]\n",
    "                \n",
    "                batch_xs=RGB_input[batch_indices]\n",
    "            # Fit training using batch data\n",
    "                cost = self.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "                avg_cost += cost/n_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "                      \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "\n",
    "##  Load Data \n",
    "RGB_data=np.load(\"../Data/rgb_data.npy\")\n",
    "R_data=RGB_data[:,:,:,0].reshape(-1,1080)\n",
    "G_data=RGB_data[:,:,:,1].reshape(-1,1080)\n",
    "B_data=RGB_data[:,:,:,2].reshape(-1,1080)\n",
    "RGB_input=np.concatenate((R_data,G_data,B_data),axis=1)\n",
    "n_samples=RGB_input.shape[0]\n",
    "\n",
    "with tf.variable_scope(\"RGB\"):\n",
    "\n",
    "    network_architecture = \\\n",
    "        dict(n_hidden_recog_1=1000, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=1000, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=1000, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=1000, # 2nd layer decoder neurons\n",
    "         n_input=3*1080, # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "    vae =VariationalAutoencoder(network_architecture, learning_rate=0.001, batch_size=100)\n",
    "\n",
    "\n",
    "    train_new_model =True\n",
    "    if train_new_model:\n",
    "        \n",
    "        vae.train(batch_size=100,training_epochs=10) \n",
    "        vae.save(\"vae_models/RGB_10_epochs/model\")\n",
    "    else:\n",
    "        \n",
    "        vae.load(\"vae_models/RGB_10_epochs/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=vae.sess.run(vae.x_reconstr_mean,feed_dict={vae.x:RGB_input[1000:1100,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_prediction=prediction[:,0:1080].reshape(-1,18,60)\n",
    "G_prediction=prediction[:,1080:2160].reshape(-1,18,60)\n",
    "B_prediction=prediction[:,2160:3240].reshape(-1,18,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18, 60)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x181db6c2e8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACHCAYAAAAGEqJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18499d9320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(RGB_data[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18367862b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACHCAYAAAAGEqJQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCl0IYSoCP8H963b2riiP6sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181da94588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(R_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
